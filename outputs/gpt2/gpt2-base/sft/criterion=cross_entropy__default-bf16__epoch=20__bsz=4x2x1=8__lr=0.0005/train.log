df: /root/.triton/autotune: No such file or directory
[2025-12-10 01:23:57] [INFO]  Using default tokenizer.
[2025-12-10 01:23:57] [INFO]  Using world size: 1


============================== EXP at 2025-12-10 01:23:57 ==============================
[2025-12-10 01:23:57] [INFO]  Namespace(model_path='/workspace/DSKD/model_hub/gpt2/gpt2-base', ckpt_name=None, model_type='gpt2', teacher_model_type=None, n_gpu=1, n_nodes=1, teacher_model_path=None, teacher_model_fp16=False, model_parallel=False, model_parallel_size=None, no_value=False, dropout_path_rate=None, fp32=False, model_dtype='fp16', task='sft', do_train=True, do_valid=True, do_eval=False, base_path='/workspace/DSKD', load=None, save_dir='/workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005', log_interval=50, save_interval=1, eval_interval=1, local_rank=0, save_additional_suffix='', save_rollout=False, eb_sample_times=3, keep_best_n_checkpoints=1, criterion='cross_entropy', eval_tqdm=False, report_logits=False, only_save_projector=False, debug=False, data_dir='/workspace/DSKD/data/dolly/', processed_data_dir=None, force_process=False, force_process_demo=False, data_process_workers=-1, train_num=-1, train_ratio=1, dev_num=1000, dev_ratio=1, gen_num=-1, data_names=None, prompt_type=None, num_workers=0, max_prompt_length=256, min_prompt_length=128, json_data=False, bin_data=False, txt_data=False, prompt_data_dir=None, pretrain_data_dir=None, eval_ppl=False, eval_rw=False, eval_gen=True, only_prompt=False, batch_size=4, eval_batch_size=32, clip_grad=1.0, total_iters=None, train_iters_per_epoch=-1, max_length=512, seed=10, seed_order=42, seed_data=42, seed_ppo=42, seed_lm=7, num_epochs=20, training_epochs=10000, gradient_accumulation_steps=2, gradient_checkpointing=False, attn_dtype=None, lr=0.0005, lr_min=1e-07, weight_decay=0.01, loss_scale=65536, kd_rate=0.5, kd_temperature=1.0, kd_objective='forward_kl', teacher_temperature=1.0, label_smoothing=0.0, adaptive_kl_alpha=0.5, skew_lambda=0.1, warmup_iters=0, lr_decay_iters=None, lr_decay_style='cosine', scheduler_name='constant_trm', top_k=0, top_p=1.0, do_sample=True, no_repeat_ngram_size=6, repetition_penalty=None, num_beams=1, temperature=1.0, eval_gen_repeat_times=3, peft=None, peft_lora_r=16, peft_lora_alpha=64, peft_lora_dropout=0.1, peft_name=None, peft_path=None, teacher_peft_name=None, teacher_peft_path=None, deepspeed=True, deepspeed_config='/workspace/DSKD/configs/deepspeed/ds_config_bf16.json', deepscale=False, deepscale_config=None, projector_config_path=None, projector_path=None, projector_lr=0.001, pretrained_projector=None, pretrained_projector_lr=0.001, vocab_alignment_path=None, teacher_to_student_token_mapping=None, teacher_to_student_id_mapping=None, student_to_teacher_token_mapping=None, student_to_teacher_id_mapping=None, rank=0, world_size=1)
[2025-12-10 01:23:57] [INFO]  Initializing a distiller for knowledge distillation...
[2025-12-10 01:23:57] [INFO]  Loading student model...
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/DSKD/code/distillation.py", line 599, in <module>
[rank0]:     main()
[rank0]:   File "/workspace/DSKD/code/distillation.py", line 545, in main
[rank0]:     distiller = Distiller(args, device)
[rank0]:                 ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/code/distiller.py", line 25, in __init__
[rank0]:     self.student_model, self.student_tokenizer = self.load_student_model()
[rank0]:                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/code/distiller.py", line 180, in load_student_model
[rank0]:     model = AutoModelForCausalLM.from_pretrained(
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
[rank0]:     return model_class.from_pretrained(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4260, in from_pretrained
[rank0]:     checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(
[rank0]:                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 952, in _get_resolved_checkpoint_files
[rank0]:     raise EnvironmentError(
[rank0]: OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory /workspace/DSKD/model_hub/gpt2/gpt2-base.
[rank0]:[W1210 01:23:58.210522126 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E1210 01:23:59.344000 5424 .venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 5539) of binary: /workspace/DSKD/.venv/bin/python
Traceback (most recent call last):
  File "/workspace/DSKD/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/workspace/DSKD/code/distillation.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-10_01:23:59
  host      : 3bcfe177ec7b
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 5539)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[2025-12-10 01:26:22] [INFO]  Using default tokenizer.
[2025-12-10 01:26:22] [INFO]  Using world size: 1


============================== EXP at 2025-12-10 01:26:22 ==============================
[2025-12-10 01:26:22] [INFO]  Namespace(model_path='/workspace/DSKD/model_hub/gpt2/gpt2-base', ckpt_name=None, model_type='gpt2', teacher_model_type=None, n_gpu=1, n_nodes=1, teacher_model_path=None, teacher_model_fp16=False, model_parallel=False, model_parallel_size=None, no_value=False, dropout_path_rate=None, fp32=False, model_dtype='fp16', task='sft', do_train=True, do_valid=True, do_eval=False, base_path='/workspace/DSKD', load=None, save_dir='/workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005', log_interval=50, save_interval=1, eval_interval=1, local_rank=0, save_additional_suffix='', save_rollout=False, eb_sample_times=3, keep_best_n_checkpoints=1, criterion='cross_entropy', eval_tqdm=False, report_logits=False, only_save_projector=False, debug=False, data_dir='/workspace/DSKD/data/dolly/', processed_data_dir=None, force_process=False, force_process_demo=False, data_process_workers=-1, train_num=-1, train_ratio=1, dev_num=1000, dev_ratio=1, gen_num=-1, data_names=None, prompt_type=None, num_workers=0, max_prompt_length=256, min_prompt_length=128, json_data=False, bin_data=False, txt_data=False, prompt_data_dir=None, pretrain_data_dir=None, eval_ppl=False, eval_rw=False, eval_gen=True, only_prompt=False, batch_size=4, eval_batch_size=32, clip_grad=1.0, total_iters=None, train_iters_per_epoch=-1, max_length=512, seed=10, seed_order=42, seed_data=42, seed_ppo=42, seed_lm=7, num_epochs=20, training_epochs=10000, gradient_accumulation_steps=2, gradient_checkpointing=False, attn_dtype=None, lr=0.0005, lr_min=1e-07, weight_decay=0.01, loss_scale=65536, kd_rate=0.5, kd_temperature=1.0, kd_objective='forward_kl', teacher_temperature=1.0, label_smoothing=0.0, adaptive_kl_alpha=0.5, skew_lambda=0.1, warmup_iters=0, lr_decay_iters=None, lr_decay_style='cosine', scheduler_name='constant_trm', top_k=0, top_p=1.0, do_sample=True, no_repeat_ngram_size=6, repetition_penalty=None, num_beams=1, temperature=1.0, eval_gen_repeat_times=3, peft=None, peft_lora_r=16, peft_lora_alpha=64, peft_lora_dropout=0.1, peft_name=None, peft_path=None, teacher_peft_name=None, teacher_peft_path=None, deepspeed=True, deepspeed_config='/workspace/DSKD/configs/deepspeed/ds_config_bf16.json', deepscale=False, deepscale_config=None, projector_config_path=None, projector_path=None, projector_lr=0.001, pretrained_projector=None, pretrained_projector_lr=0.001, vocab_alignment_path=None, teacher_to_student_token_mapping=None, teacher_to_student_id_mapping=None, student_to_teacher_token_mapping=None, student_to_teacher_id_mapping=None, rank=0, world_size=1)
[2025-12-10 01:26:22] [INFO]  Initializing a distiller for knowledge distillation...
[2025-12-10 01:26:22] [INFO]  Loading student model...
[2025-12-10 01:26:22] [INFO]   > number of parameters: 124,439,808
[2025-12-10 01:26:22] [INFO]  Processing dataset for student model (and all teacher models)...
  0%|          | 0/11435 [00:00<?, ?it/s]  2%|▏         | 213/11435 [00:00<00:05, 2121.45it/s]  4%|▍         | 465/11435 [00:00<00:04, 2353.10it/s]  7%|▋         | 750/11435 [00:00<00:04, 2577.69it/s]  9%|▉         | 1026/11435 [00:00<00:03, 2644.00it/s] 11%|█▏        | 1303/11435 [00:00<00:03, 2688.63it/s] 14%|█▎        | 1572/11435 [00:00<00:03, 2683.67it/s] 16%|█▌        | 1841/11435 [00:00<00:03, 2657.27it/s] 19%|█▊        | 2117/11435 [00:00<00:03, 2686.19it/s] 21%|██        | 2395/11435 [00:00<00:03, 2714.12it/s] 23%|██▎       | 2667/11435 [00:01<00:03, 2706.26it/s] 26%|██▌       | 2941/11435 [00:01<00:03, 2715.24it/s] 28%|██▊       | 3222/11435 [00:01<00:02, 2738.17it/s] 31%|███       | 3496/11435 [00:01<00:02, 2656.55it/s] 33%|███▎      | 3767/11435 [00:01<00:02, 2671.96it/s] 35%|███▌      | 4039/11435 [00:01<00:02, 2683.20it/s] 38%|███▊      | 4309/11435 [00:01<00:02, 2686.37it/s] 40%|████      | 4578/11435 [00:01<00:02, 2656.04it/s] 42%|████▏     | 4849/11435 [00:01<00:02, 2671.45it/s] 45%|████▍     | 5117/11435 [00:01<00:02, 2614.81it/s] 47%|████▋     | 5379/11435 [00:02<00:02, 2591.09it/s] 49%|████▉     | 5647/11435 [00:02<00:02, 2616.45it/s] 52%|█████▏    | 5924/11435 [00:02<00:02, 2661.74it/s] 54%|█████▍    | 6192/11435 [00:02<00:01, 2665.26it/s] 56%|█████▋    | 6459/11435 [00:02<00:01, 2646.24it/s] 59%|█████▉    | 6724/11435 [00:02<00:01, 2634.21it/s] 61%|██████    | 6995/11435 [00:02<00:01, 2654.67it/s] 64%|██████▎   | 7264/11435 [00:02<00:01, 2665.03it/s] 66%|██████▌   | 7531/11435 [00:02<00:01, 2567.82it/s] 68%|██████▊   | 7803/11435 [00:02<00:01, 2610.47it/s] 71%|███████   | 8065/11435 [00:03<00:01, 2593.71it/s] 73%|███████▎  | 8325/11435 [00:03<00:01, 2540.60it/s] 75%|███████▌  | 8586/11435 [00:03<00:01, 2559.91it/s] 78%|███████▊  | 8863/11435 [00:03<00:00, 2620.42it/s] 80%|███████▉  | 9126/11435 [00:03<00:00, 2582.80it/s] 82%|████████▏ | 9387/11435 [00:03<00:00, 2589.77it/s] 84%|████████▍ | 9647/11435 [00:03<00:00, 2585.23it/s] 87%|████████▋ | 9906/11435 [00:03<00:00, 2559.44it/s] 89%|████████▉ | 10186/11435 [00:03<00:00, 2629.30it/s] 91%|█████████▏| 10452/11435 [00:03<00:00, 2637.51it/s] 94%|█████████▎| 10716/11435 [00:04<00:00, 2582.35it/s] 96%|█████████▌| 10990/11435 [00:04<00:00, 2627.71it/s] 99%|█████████▊| 11266/11435 [00:04<00:00, 2662.47it/s]100%|██████████| 11435/11435 [00:04<00:00, 2633.01it/s]
[2025-12-10 01:26:27] [INFO]  Num of train data: 11435
[2025-12-10 01:26:27] [INFO]  Processing dataset for student model (and all teacher models)...
  0%|          | 0/1000 [00:00<?, ?it/s] 24%|██▎       | 236/1000 [00:00<00:00, 2354.69it/s] 48%|████▊     | 485/1000 [00:00<00:00, 2432.13it/s] 75%|███████▌  | 751/1000 [00:00<00:00, 2531.50it/s]100%|██████████| 1000/1000 [00:00<00:00, 2533.73it/s]
[2025-12-10 01:26:27] [INFO]  Num of dev data: 1000
[2025-12-10 01:26:27] [INFO]  Train iters per epoch = 1429
[2025-12-10 01:26:27] [INFO]  Total_iters = 28580
[2025-12-10 01:26:27] [INFO]  Optimizer = AdamW
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/DSKD/code/distillation.py", line 599, in <module>
[rank0]:     main()
[rank0]:   File "/workspace/DSKD/code/distillation.py", line 574, in main
[rank0]:     model, optimizer, _, lr_scheduler = deepspeed.initialize(
[rank0]:                                         ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/deepspeed/__init__.py", line 188, in initialize
[rank0]:     config_class = DeepSpeedConfig(config, mpu, mesh_device=mesh_device)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/deepspeed/runtime/config.py", line 751, in __init__
[rank0]:     self._initialize_params(copy.copy(self._param_dict))
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/deepspeed/runtime/config.py", line 783, in _initialize_params
[rank0]:     self.bfloat16_config = get_bfloat16_config(param_dict)
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/deepspeed/runtime/precision_config.py", line 36, in get_bfloat16_config
[rank0]:     return DeepSpeedBF16Config(**bf16_config_dict)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/deepspeed/runtime/config_utils.py", line 57, in __init__
[rank0]:     super().__init__(**data)
[rank0]:   File "/workspace/DSKD/.venv/lib/python3.11/site-packages/pydantic/main.py", line 253, in __init__
[rank0]:     validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: pydantic_core._pydantic_core.ValidationError: 4 validation errors for DeepSpeedBF16Config
[rank0]: loss_scale
[rank0]:   Extra inputs are not permitted [type=extra_forbidden, input_value=0, input_type=int]
[rank0]:     For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden
[rank0]: initial_scale_power
[rank0]:   Extra inputs are not permitted [type=extra_forbidden, input_value=11, input_type=int]
[rank0]:     For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden
[rank0]: loss_scale_window
[rank0]:   Extra inputs are not permitted [type=extra_forbidden, input_value=2000, input_type=int]
[rank0]:     For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden
[rank0]: hysteresis
[rank0]:   Extra inputs are not permitted [type=extra_forbidden, input_value=4, input_type=int]
[rank0]:     For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden
[rank0]:[W1210 01:26:28.221363264 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
E1210 01:26:29.143000 6521 .venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 6601) of binary: /workspace/DSKD/.venv/bin/python
Traceback (most recent call last):
  File "/workspace/DSKD/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/DSKD/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/workspace/DSKD/code/distillation.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-10_01:26:29
  host      : 3bcfe177ec7b
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 6601)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
[2025-12-10 01:31:57] [INFO]  Using default tokenizer.
[2025-12-10 01:31:57] [INFO]  Using world size: 1


============================== EXP at 2025-12-10 01:31:57 ==============================
[2025-12-10 01:31:57] [INFO]  Namespace(model_path='/workspace/DSKD/model_hub/gpt2/gpt2-base', ckpt_name=None, model_type='gpt2', teacher_model_type=None, n_gpu=1, n_nodes=1, teacher_model_path=None, teacher_model_fp16=False, model_parallel=False, model_parallel_size=None, no_value=False, dropout_path_rate=None, fp32=False, model_dtype='fp16', task='sft', do_train=True, do_valid=True, do_eval=False, base_path='/workspace/DSKD', load=None, save_dir='/workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005', log_interval=50, save_interval=1, eval_interval=1, local_rank=0, save_additional_suffix='', save_rollout=False, eb_sample_times=3, keep_best_n_checkpoints=1, criterion='cross_entropy', eval_tqdm=False, report_logits=False, only_save_projector=False, debug=False, data_dir='/workspace/DSKD/data/dolly/', processed_data_dir=None, force_process=False, force_process_demo=False, data_process_workers=-1, train_num=-1, train_ratio=1, dev_num=1000, dev_ratio=1, gen_num=-1, data_names=None, prompt_type=None, num_workers=0, max_prompt_length=256, min_prompt_length=128, json_data=False, bin_data=False, txt_data=False, prompt_data_dir=None, pretrain_data_dir=None, eval_ppl=False, eval_rw=False, eval_gen=True, only_prompt=False, batch_size=4, eval_batch_size=32, clip_grad=1.0, total_iters=None, train_iters_per_epoch=-1, max_length=512, seed=10, seed_order=42, seed_data=42, seed_ppo=42, seed_lm=7, num_epochs=20, training_epochs=10000, gradient_accumulation_steps=2, gradient_checkpointing=False, attn_dtype=None, lr=0.0005, lr_min=1e-07, weight_decay=0.01, loss_scale=65536, kd_rate=0.5, kd_temperature=1.0, kd_objective='forward_kl', teacher_temperature=1.0, label_smoothing=0.0, adaptive_kl_alpha=0.5, skew_lambda=0.1, warmup_iters=0, lr_decay_iters=None, lr_decay_style='cosine', scheduler_name='constant_trm', top_k=0, top_p=1.0, do_sample=True, no_repeat_ngram_size=6, repetition_penalty=None, num_beams=1, temperature=1.0, eval_gen_repeat_times=3, peft=None, peft_lora_r=16, peft_lora_alpha=64, peft_lora_dropout=0.1, peft_name=None, peft_path=None, teacher_peft_name=None, teacher_peft_path=None, deepspeed=True, deepspeed_config='/workspace/DSKD/configs/deepspeed/ds_config_bf16.json', deepscale=False, deepscale_config=None, projector_config_path=None, projector_path=None, projector_lr=0.001, pretrained_projector=None, pretrained_projector_lr=0.001, vocab_alignment_path=None, teacher_to_student_token_mapping=None, teacher_to_student_id_mapping=None, student_to_teacher_token_mapping=None, student_to_teacher_id_mapping=None, rank=0, world_size=1)
[2025-12-10 01:31:57] [INFO]  Initializing a distiller for knowledge distillation...
[2025-12-10 01:31:57] [INFO]  Loading student model...
[2025-12-10 01:31:58] [INFO]   > number of parameters: 124,439,808
[2025-12-10 01:31:58] [INFO]  Processing dataset for student model (and all teacher models)...
  0%|          | 0/11435 [00:00<?, ?it/s]  2%|▏         | 229/11435 [00:00<00:04, 2287.29it/s]  4%|▍         | 474/11435 [00:00<00:04, 2378.16it/s]  7%|▋         | 755/11435 [00:00<00:04, 2571.83it/s]  9%|▉         | 1038/11435 [00:00<00:03, 2671.71it/s] 12%|█▏        | 1319/11435 [00:00<00:03, 2720.68it/s] 14%|█▍        | 1592/11435 [00:00<00:03, 2667.91it/s] 16%|█▋        | 1859/11435 [00:00<00:03, 2664.29it/s] 19%|█▊        | 2134/11435 [00:00<00:03, 2688.14it/s] 21%|██        | 2413/11435 [00:00<00:03, 2717.03it/s] 23%|██▎       | 2685/11435 [00:01<00:03, 2690.76it/s] 26%|██▌       | 2956/11435 [00:01<00:03, 2693.78it/s] 28%|██▊       | 3230/11435 [00:01<00:03, 2698.69it/s] 31%|███       | 3500/11435 [00:01<00:02, 2657.07it/s] 33%|███▎      | 3772/11435 [00:01<00:02, 2674.23it/s] 35%|███▌      | 4040/11435 [00:01<00:02, 2675.46it/s] 38%|███▊      | 4309/11435 [00:01<00:02, 2678.91it/s] 40%|████      | 4577/11435 [00:01<00:02, 2647.59it/s] 42%|████▏     | 4849/11435 [00:01<00:02, 2666.29it/s] 45%|████▍     | 5116/11435 [00:01<00:02, 2607.37it/s] 47%|████▋     | 5378/11435 [00:02<00:02, 2582.06it/s] 49%|████▉     | 5646/11435 [00:02<00:02, 2608.91it/s] 52%|█████▏    | 5921/11435 [00:02<00:02, 2649.23it/s] 54%|█████▍    | 6187/11435 [00:02<00:01, 2649.74it/s] 56%|█████▋    | 6453/11435 [00:02<00:01, 2640.24it/s] 59%|█████▉    | 6721/11435 [00:02<00:01, 2650.69it/s] 61%|██████    | 6987/11435 [00:02<00:01, 2632.71it/s] 63%|██████▎   | 7255/11435 [00:02<00:01, 2643.27it/s] 66%|██████▌   | 7520/11435 [00:02<00:01, 2566.90it/s] 68%|██████▊   | 7789/11435 [00:02<00:01, 2597.90it/s] 70%|███████   | 8050/11435 [00:03<00:01, 2579.19it/s] 73%|███████▎  | 8309/11435 [00:03<00:01, 2534.44it/s] 75%|███████▍  | 8576/11435 [00:03<00:01, 2573.25it/s] 77%|███████▋  | 8850/11435 [00:03<00:00, 2619.13it/s] 80%|███████▉  | 9113/11435 [00:03<00:00, 2506.84it/s] 82%|████████▏ | 9372/11435 [00:03<00:00, 2528.63it/s] 84%|████████▍ | 9626/11435 [00:03<00:00, 2520.63it/s] 87%|████████▋ | 9897/11435 [00:03<00:00, 2574.52it/s] 89%|████████▉ | 10185/11435 [00:03<00:00, 2660.77it/s] 91%|█████████▏| 10452/11435 [00:03<00:00, 2663.33it/s] 94%|█████████▎| 10719/11435 [00:04<00:00, 2599.92it/s] 96%|█████████▌| 10992/11435 [00:04<00:00, 2637.87it/s] 99%|█████████▊| 11266/11435 [00:04<00:00, 2665.89it/s]100%|██████████| 11435/11435 [00:04<00:00, 2628.27it/s]
[2025-12-10 01:32:02] [INFO]  Num of train data: 11435
[2025-12-10 01:32:02] [INFO]  Processing dataset for student model (and all teacher models)...
  0%|          | 0/1000 [00:00<?, ?it/s] 24%|██▎       | 236/1000 [00:00<00:00, 2347.99it/s] 48%|████▊     | 483/1000 [00:00<00:00, 2419.33it/s] 74%|███████▍  | 744/1000 [00:00<00:00, 2505.58it/s]100%|██████████| 1000/1000 [00:00<00:00, 2522.27it/s]
[2025-12-10 01:32:03] [INFO]  Num of dev data: 1000
[2025-12-10 01:32:03] [INFO]  Train iters per epoch = 1429
[2025-12-10 01:32:03] [INFO]  Total_iters = 28580
[2025-12-10 01:32:03] [INFO]  Optimizer = AdamW
Before initializing optimizer states
MA 0.71 GB         Max_MA 0.71 GB         CA 0.71 GB         Max_CA 1 GB 
CPU Virtual Memory:  used = 35.36 GB, percent = 7.0%
After initializing optimizer states
MA 0.71 GB         Max_MA 1.17 GB         CA 1.18 GB         Max_CA 1 GB 
CPU Virtual Memory:  used = 35.36 GB, percent = 7.0%
After initializing ZeRO optimizer
MA 0.71 GB         Max_MA 0.71 GB         CA 1.18 GB         Max_CA 1 GB 
CPU Virtual Memory:  used = 35.36 GB, percent = 7.0%
[2025-12-10 01:32:05] [INFO]  Start Fine-tuning
[2025-12-10 01:32:05] [INFO]  Start iterations of epoch 1
[2025-12-10 01:32:11] [INFO]  train | epoch 001:      50 /  1429  global_step=50, loss=3.3072, nll_loss=3.3072, accuracy=0.3871, micro_step_time=0.0660, step_time=0.1387, lr=5.0000e-04, scale=1.0000
[2025-12-10 01:32:18] [INFO]  train | epoch 001:     100 /  1429  global_step=100, loss=3.2034, nll_loss=3.2034, accuracy=0.4040, micro_step_time=0.0613, step_time=0.1254, lr=4.9998e-04, scale=1.0000
[2025-12-10 01:32:24] [INFO]  train | epoch 001:     150 /  1429  global_step=150, loss=3.3286, nll_loss=3.3286, accuracy=0.3971, micro_step_time=0.0611, step_time=0.1249, lr=4.9997e-04, scale=1.0000
[2025-12-10 01:32:30] [INFO]  train | epoch 001:     200 /  1429  global_step=200, loss=3.2321, nll_loss=3.2321, accuracy=0.4062, micro_step_time=0.0613, step_time=0.1252, lr=4.9994e-04, scale=1.0000
[2025-12-10 01:32:37] [INFO]  train | epoch 001:     250 /  1429  global_step=250, loss=3.2626, nll_loss=3.2626, accuracy=0.4031, micro_step_time=0.0613, step_time=0.1254, lr=4.9991e-04, scale=1.0000
[2025-12-10 01:32:43] [INFO]  train | epoch 001:     300 /  1429  global_step=300, loss=3.3946, nll_loss=3.3946, accuracy=0.3830, micro_step_time=0.0613, step_time=0.1254, lr=4.9986e-04, scale=1.0000
[2025-12-10 01:32:49] [INFO]  train | epoch 001:     350 /  1429  global_step=350, loss=3.2840, nll_loss=3.2840, accuracy=0.3974, micro_step_time=0.0613, step_time=0.1253, lr=4.9982e-04, scale=1.0000
[2025-12-10 01:32:55] [INFO]  train | epoch 001:     400 /  1429  global_step=400, loss=3.2237, nll_loss=3.2237, accuracy=0.4113, micro_step_time=0.0614, step_time=0.1255, lr=4.9976e-04, scale=1.0000
[2025-12-10 01:33:02] [INFO]  train | epoch 001:     450 /  1429  global_step=450, loss=3.3301, nll_loss=3.3301, accuracy=0.3957, micro_step_time=0.0613, step_time=0.1253, lr=4.9969e-04, scale=1.0000
[2025-12-10 01:33:08] [INFO]  train | epoch 001:     500 /  1429  global_step=500, loss=3.3445, nll_loss=3.3445, accuracy=0.3978, micro_step_time=0.0612, step_time=0.1252, lr=4.9962e-04, scale=1.0000
[2025-12-10 01:33:14] [INFO]  train | epoch 001:     550 /  1429  global_step=550, loss=3.3173, nll_loss=3.3173, accuracy=0.3957, micro_step_time=0.0612, step_time=0.1252, lr=4.9954e-04, scale=1.0000
[2025-12-10 01:33:20] [INFO]  train | epoch 001:     600 /  1429  global_step=600, loss=3.2705, nll_loss=3.2705, accuracy=0.4054, micro_step_time=0.0613, step_time=0.1254, lr=4.9946e-04, scale=1.0000
[2025-12-10 01:33:27] [INFO]  train | epoch 001:     650 /  1429  global_step=650, loss=3.3696, nll_loss=3.3696, accuracy=0.3890, micro_step_time=0.0612, step_time=0.1252, lr=4.9936e-04, scale=1.0000
[2025-12-10 01:33:33] [INFO]  train | epoch 001:     700 /  1429  global_step=700, loss=3.3312, nll_loss=3.3312, accuracy=0.3950, micro_step_time=0.0613, step_time=0.1254, lr=4.9926e-04, scale=1.0000
[2025-12-10 01:33:39] [INFO]  train | epoch 001:     750 /  1429  global_step=750, loss=3.3396, nll_loss=3.3396, accuracy=0.3986, micro_step_time=0.0613, step_time=0.1254, lr=4.9915e-04, scale=1.0000
[2025-12-10 01:33:45] [INFO]  train | epoch 001:     800 /  1429  global_step=800, loss=3.3076, nll_loss=3.3076, accuracy=0.4057, micro_step_time=0.0612, step_time=0.1252, lr=4.9903e-04, scale=1.0000
[2025-12-10 01:33:52] [INFO]  train | epoch 001:     850 /  1429  global_step=850, loss=3.2473, nll_loss=3.2473, accuracy=0.4174, micro_step_time=0.0612, step_time=0.1252, lr=4.9891e-04, scale=1.0000
[2025-12-10 01:33:58] [INFO]  train | epoch 001:     900 /  1429  global_step=900, loss=3.2863, nll_loss=3.2863, accuracy=0.4078, micro_step_time=0.0613, step_time=0.1253, lr=4.9878e-04, scale=1.0000
[2025-12-10 01:34:04] [INFO]  train | epoch 001:     950 /  1429  global_step=950, loss=3.1826, nll_loss=3.1826, accuracy=0.4248, micro_step_time=0.0612, step_time=0.1250, lr=4.9864e-04, scale=1.0000
[2025-12-10 01:34:10] [INFO]  train | epoch 001:    1000 /  1429  global_step=1000, loss=3.3557, nll_loss=3.3557, accuracy=0.3941, micro_step_time=0.0612, step_time=0.1250, lr=4.9849e-04, scale=1.0000
[2025-12-10 01:34:17] [INFO]  train | epoch 001:    1050 /  1429  global_step=1050, loss=3.1957, nll_loss=3.1957, accuracy=0.4212, micro_step_time=0.0612, step_time=0.1251, lr=4.9834e-04, scale=1.0000
[2025-12-10 01:34:23] [INFO]  train | epoch 001:    1100 /  1429  global_step=1100, loss=3.3471, nll_loss=3.3471, accuracy=0.3932, micro_step_time=0.0613, step_time=0.1253, lr=4.9818e-04, scale=1.0000
[2025-12-10 01:34:29] [INFO]  train | epoch 001:    1150 /  1429  global_step=1150, loss=3.3259, nll_loss=3.3259, accuracy=0.3985, micro_step_time=0.0613, step_time=0.1252, lr=4.9801e-04, scale=1.0000
[2025-12-10 01:34:36] [INFO]  train | epoch 001:    1200 /  1429  global_step=1200, loss=3.2326, nll_loss=3.2326, accuracy=0.4144, micro_step_time=0.0612, step_time=0.1250, lr=4.9783e-04, scale=1.0000
[2025-12-10 01:34:42] [INFO]  train | epoch 001:    1250 /  1429  global_step=1250, loss=3.3831, nll_loss=3.3831, accuracy=0.3868, micro_step_time=0.0612, step_time=0.1252, lr=4.9764e-04, scale=1.0000
[2025-12-10 01:34:48] [INFO]  train | epoch 001:    1300 /  1429  global_step=1300, loss=3.2033, nll_loss=3.2033, accuracy=0.4100, micro_step_time=0.0613, step_time=0.1252, lr=4.9745e-04, scale=1.0000
[2025-12-10 01:34:54] [INFO]  train | epoch 001:    1350 /  1429  global_step=1350, loss=3.1394, nll_loss=3.1394, accuracy=0.4333, micro_step_time=0.0612, step_time=0.1251, lr=4.9725e-04, scale=1.0000
[2025-12-10 01:35:01] [INFO]  train | epoch 001:    1400 /  1429  global_step=1400, loss=3.3557, nll_loss=3.3557, accuracy=0.3941, micro_step_time=0.0612, step_time=0.1252, lr=4.9705e-04, scale=1.0000
[2025-12-10 01:35:04] [INFO]  End of epoch 1
[2025-12-10 01:35:04] [INFO]  train | epoch 001 | loss 3.2226 | nll_loss 3.2226 | kd_loss 0.0000
[2025-12-10 01:35:04] [INFO]  Evaluating before saving model...
[2025-12-10 01:35:04] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 01:36:14] [INFO]  eval_results in run@1: {'exact_match': 2.1, 'rougeL': 15.5375}
[2025-12-10 01:37:21] [INFO]  eval_results in run@2: {'exact_match': 1.6, 'rougeL': 15.9218}
[2025-12-10 01:38:29] [INFO]  eval_results in run@3: {'exact_match': 1.7, 'rougeL': 15.4714}
[2025-12-10 01:38:29] [INFO]  dev | {'loss': 3.091391, 'token_num': 75795, 'token_acc': 0.424078, 'top1_prob': 0.456455} | {'exact_match': 1.8, 'rougeL': 15.6436}
[2025-12-10 01:38:29] [INFO]  Saving tokenizer...
[2025-12-10 01:38:29] [INFO]  Saving model...
[2025-12-10 01:38:30] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch1_step1429_loss3.0914_rougel15.6436
[2025-12-10 01:38:30] [INFO]  Start iterations of epoch 2
[2025-12-10 01:38:32] [INFO]  train | epoch 002:      21 /  1429  global_step=1450, loss=3.0440, nll_loss=3.0440, accuracy=0.4254, micro_step_time=0.0610, step_time=0.1248, lr=4.9683e-04, scale=1.0000
[2025-12-10 01:38:39] [INFO]  train | epoch 002:      71 /  1429  global_step=1500, loss=2.5168, nll_loss=2.5168, accuracy=0.4810, micro_step_time=0.0615, step_time=0.1258, lr=4.9661e-04, scale=1.0000
[2025-12-10 01:38:45] [INFO]  train | epoch 002:     121 /  1429  global_step=1550, loss=2.4328, nll_loss=2.4328, accuracy=0.4922, micro_step_time=0.0614, step_time=0.1255, lr=4.9638e-04, scale=1.0000
[2025-12-10 01:38:51] [INFO]  train | epoch 002:     171 /  1429  global_step=1600, loss=2.5134, nll_loss=2.5134, accuracy=0.4773, micro_step_time=0.0614, step_time=0.1256, lr=4.9614e-04, scale=1.0000
[2025-12-10 01:38:58] [INFO]  train | epoch 002:     221 /  1429  global_step=1650, loss=2.4617, nll_loss=2.4617, accuracy=0.4906, micro_step_time=0.0614, step_time=0.1255, lr=4.9590e-04, scale=1.0000
[2025-12-10 01:39:04] [INFO]  train | epoch 002:     271 /  1429  global_step=1700, loss=2.5831, nll_loss=2.5831, accuracy=0.4758, micro_step_time=0.0614, step_time=0.1256, lr=4.9565e-04, scale=1.0000
[2025-12-10 01:39:10] [INFO]  train | epoch 002:     321 /  1429  global_step=1750, loss=2.5816, nll_loss=2.5816, accuracy=0.4738, micro_step_time=0.0614, step_time=0.1256, lr=4.9539e-04, scale=1.0000
[2025-12-10 01:39:16] [INFO]  train | epoch 002:     371 /  1429  global_step=1800, loss=2.5383, nll_loss=2.5383, accuracy=0.4816, micro_step_time=0.0614, step_time=0.1256, lr=4.9512e-04, scale=1.0000
[2025-12-10 01:39:23] [INFO]  train | epoch 002:     421 /  1429  global_step=1850, loss=2.6089, nll_loss=2.6089, accuracy=0.4685, micro_step_time=0.0614, step_time=0.1255, lr=4.9485e-04, scale=1.0000
[2025-12-10 01:39:29] [INFO]  train | epoch 002:     471 /  1429  global_step=1900, loss=2.5331, nll_loss=2.5331, accuracy=0.4796, micro_step_time=0.0614, step_time=0.1254, lr=4.9457e-04, scale=1.0000
[2025-12-10 01:39:35] [INFO]  train | epoch 002:     521 /  1429  global_step=1950, loss=2.5979, nll_loss=2.5979, accuracy=0.4751, micro_step_time=0.0614, step_time=0.1255, lr=4.9428e-04, scale=1.0000
[2025-12-10 01:39:42] [INFO]  train | epoch 002:     571 /  1429  global_step=2000, loss=2.6185, nll_loss=2.6185, accuracy=0.4704, micro_step_time=0.0615, step_time=0.1256, lr=4.9398e-04, scale=1.0000
[2025-12-10 01:39:48] [INFO]  train | epoch 002:     621 /  1429  global_step=2050, loss=2.5917, nll_loss=2.5917, accuracy=0.4747, micro_step_time=0.0614, step_time=0.1256, lr=4.9368e-04, scale=1.0000
[2025-12-10 01:39:54] [INFO]  train | epoch 002:     671 /  1429  global_step=2100, loss=2.6456, nll_loss=2.6456, accuracy=0.4657, micro_step_time=0.0614, step_time=0.1254, lr=4.9337e-04, scale=1.0000
[2025-12-10 01:40:00] [INFO]  train | epoch 002:     721 /  1429  global_step=2150, loss=2.5991, nll_loss=2.5991, accuracy=0.4697, micro_step_time=0.0615, step_time=0.1256, lr=4.9305e-04, scale=1.0000
[2025-12-10 01:40:07] [INFO]  train | epoch 002:     771 /  1429  global_step=2200, loss=2.6014, nll_loss=2.6014, accuracy=0.4662, micro_step_time=0.0614, step_time=0.1255, lr=4.9273e-04, scale=1.0000
[2025-12-10 01:40:13] [INFO]  train | epoch 002:     821 /  1429  global_step=2250, loss=2.6324, nll_loss=2.6324, accuracy=0.4696, micro_step_time=0.0615, step_time=0.1257, lr=4.9239e-04, scale=1.0000
[2025-12-10 01:40:19] [INFO]  train | epoch 002:     871 /  1429  global_step=2300, loss=2.6564, nll_loss=2.6564, accuracy=0.4637, micro_step_time=0.0614, step_time=0.1256, lr=4.9205e-04, scale=1.0000
[2025-12-10 01:40:25] [INFO]  train | epoch 002:     921 /  1429  global_step=2350, loss=2.7633, nll_loss=2.7633, accuracy=0.4415, micro_step_time=0.0615, step_time=0.1258, lr=4.9171e-04, scale=1.0000
[2025-12-10 01:40:32] [INFO]  train | epoch 002:     971 /  1429  global_step=2400, loss=2.5861, nll_loss=2.5861, accuracy=0.4843, micro_step_time=0.0615, step_time=0.1256, lr=4.9135e-04, scale=1.0000
[2025-12-10 01:40:38] [INFO]  train | epoch 002:    1021 /  1429  global_step=2450, loss=2.6442, nll_loss=2.6442, accuracy=0.4706, micro_step_time=0.0614, step_time=0.1256, lr=4.9099e-04, scale=1.0000
[2025-12-10 01:40:44] [INFO]  train | epoch 002:    1071 /  1429  global_step=2500, loss=2.6995, nll_loss=2.6995, accuracy=0.4630, micro_step_time=0.0618, step_time=0.1264, lr=4.9062e-04, scale=1.0000
[2025-12-10 01:40:50] [INFO]  train | epoch 002:    1121 /  1429  global_step=2550, loss=2.5995, nll_loss=2.5995, accuracy=0.4733, micro_step_time=0.0600, step_time=0.1226, lr=4.9024e-04, scale=1.0000
[2025-12-10 01:40:57] [INFO]  train | epoch 002:    1171 /  1429  global_step=2600, loss=2.7067, nll_loss=2.7067, accuracy=0.4572, micro_step_time=0.0611, step_time=0.1248, lr=4.8986e-04, scale=1.0000
[2025-12-10 01:41:03] [INFO]  train | epoch 002:    1221 /  1429  global_step=2650, loss=2.6134, nll_loss=2.6134, accuracy=0.4722, micro_step_time=0.0615, step_time=0.1257, lr=4.8947e-04, scale=1.0000
[2025-12-10 01:41:09] [INFO]  train | epoch 002:    1271 /  1429  global_step=2700, loss=2.6270, nll_loss=2.6270, accuracy=0.4719, micro_step_time=0.0614, step_time=0.1255, lr=4.8907e-04, scale=1.0000
[2025-12-10 01:41:16] [INFO]  train | epoch 002:    1321 /  1429  global_step=2750, loss=2.6225, nll_loss=2.6225, accuracy=0.4680, micro_step_time=0.0616, step_time=0.1258, lr=4.8867e-04, scale=1.0000
[2025-12-10 01:41:22] [INFO]  train | epoch 002:    1371 /  1429  global_step=2800, loss=2.8136, nll_loss=2.8136, accuracy=0.4425, micro_step_time=0.0615, step_time=0.1257, lr=4.8825e-04, scale=1.0000
[2025-12-10 01:41:28] [INFO]  train | epoch 002:    1421 /  1429  global_step=2850, loss=2.7914, nll_loss=2.7914, accuracy=0.4481, micro_step_time=0.0614, step_time=0.1256, lr=4.8783e-04, scale=1.0000
[2025-12-10 01:41:29] [INFO]  End of epoch 2
[2025-12-10 01:41:29] [INFO]  train | epoch 002 | loss 2.6670 | nll_loss 2.6670 | kd_loss 0.0000
[2025-12-10 01:41:29] [INFO]  Evaluating before saving model...
[2025-12-10 01:41:29] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 01:42:36] [INFO]  eval_results in run@1: {'exact_match': 2.1, 'rougeL': 17.2939}
[2025-12-10 01:43:43] [INFO]  eval_results in run@2: {'exact_match': 2.1, 'rougeL': 17.8969}
[2025-12-10 01:44:49] [INFO]  eval_results in run@3: {'exact_match': 2.1, 'rougeL': 17.4495}
[2025-12-10 01:44:49] [INFO]  dev | {'loss': 3.174352, 'token_num': 75795, 'token_acc': 0.42231, 'top1_prob': 0.510799} | {'exact_match': 2.1, 'rougeL': 17.5468}
[2025-12-10 01:44:49] [INFO]  Saving tokenizer...
[2025-12-10 01:44:49] [INFO]  Saving model...
[2025-12-10 01:44:50] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch2_step2858_loss3.1744_rougel17.5468
[2025-12-10 01:44:50] [INFO]  Start iterations of epoch 3
[2025-12-10 01:44:55] [INFO]  train | epoch 003:      42 /  1429  global_step=2900, loss=2.0026, nll_loss=2.0026, accuracy=0.5580, micro_step_time=0.0612, step_time=0.1251, lr=4.8741e-04, scale=1.0000
[2025-12-10 01:45:01] [INFO]  train | epoch 003:      92 /  1429  global_step=2950, loss=1.9169, nll_loss=1.9169, accuracy=0.5643, micro_step_time=0.0615, step_time=0.1258, lr=4.8697e-04, scale=1.0000
[2025-12-10 01:45:08] [INFO]  train | epoch 003:     142 /  1429  global_step=3000, loss=1.9805, nll_loss=1.9805, accuracy=0.5474, micro_step_time=0.0615, step_time=0.1258, lr=4.8653e-04, scale=1.0000
[2025-12-10 01:45:14] [INFO]  train | epoch 003:     192 /  1429  global_step=3050, loss=1.9578, nll_loss=1.9578, accuracy=0.5535, micro_step_time=0.0614, step_time=0.1256, lr=4.8608e-04, scale=1.0000
[2025-12-10 01:45:20] [INFO]  train | epoch 003:     242 /  1429  global_step=3100, loss=2.0023, nll_loss=2.0023, accuracy=0.5511, micro_step_time=0.0615, step_time=0.1256, lr=4.8563e-04, scale=1.0000
[2025-12-10 01:45:27] [INFO]  train | epoch 003:     292 /  1429  global_step=3150, loss=1.9110, nll_loss=1.9110, accuracy=0.5661, micro_step_time=0.0614, step_time=0.1256, lr=4.8517e-04, scale=1.0000
[2025-12-10 01:45:33] [INFO]  train | epoch 003:     342 /  1429  global_step=3200, loss=2.0044, nll_loss=2.0044, accuracy=0.5444, micro_step_time=0.0614, step_time=0.1255, lr=4.8470e-04, scale=1.0000
[2025-12-10 01:45:39] [INFO]  train | epoch 003:     392 /  1429  global_step=3250, loss=2.0082, nll_loss=2.0082, accuracy=0.5443, micro_step_time=0.0614, step_time=0.1255, lr=4.8422e-04, scale=1.0000
[2025-12-10 01:45:45] [INFO]  train | epoch 003:     442 /  1429  global_step=3300, loss=2.0558, nll_loss=2.0558, accuracy=0.5419, micro_step_time=0.0614, step_time=0.1255, lr=4.8373e-04, scale=1.0000
[2025-12-10 01:45:52] [INFO]  train | epoch 003:     492 /  1429  global_step=3350, loss=1.9986, nll_loss=1.9986, accuracy=0.5528, micro_step_time=0.0614, step_time=0.1254, lr=4.8324e-04, scale=1.0000
[2025-12-10 01:45:58] [INFO]  train | epoch 003:     542 /  1429  global_step=3400, loss=2.1148, nll_loss=2.1148, accuracy=0.5289, micro_step_time=0.0614, step_time=0.1255, lr=4.8275e-04, scale=1.0000
[2025-12-10 01:46:04] [INFO]  train | epoch 003:     592 /  1429  global_step=3450, loss=1.9620, nll_loss=1.9620, accuracy=0.5575, micro_step_time=0.0614, step_time=0.1255, lr=4.8224e-04, scale=1.0000
[2025-12-10 01:46:10] [INFO]  train | epoch 003:     642 /  1429  global_step=3500, loss=2.0789, nll_loss=2.0789, accuracy=0.5347, micro_step_time=0.0614, step_time=0.1255, lr=4.8173e-04, scale=1.0000
[2025-12-10 01:46:17] [INFO]  train | epoch 003:     692 /  1429  global_step=3550, loss=2.1697, nll_loss=2.1697, accuracy=0.5206, micro_step_time=0.0614, step_time=0.1255, lr=4.8121e-04, scale=1.0000
[2025-12-10 01:46:23] [INFO]  train | epoch 003:     742 /  1429  global_step=3600, loss=2.1331, nll_loss=2.1331, accuracy=0.5267, micro_step_time=0.0614, step_time=0.1254, lr=4.8068e-04, scale=1.0000
[2025-12-10 01:46:29] [INFO]  train | epoch 003:     792 /  1429  global_step=3650, loss=2.0235, nll_loss=2.0235, accuracy=0.5476, micro_step_time=0.0614, step_time=0.1254, lr=4.8015e-04, scale=1.0000
[2025-12-10 01:46:36] [INFO]  train | epoch 003:     842 /  1429  global_step=3700, loss=2.0687, nll_loss=2.0687, accuracy=0.5339, micro_step_time=0.0614, step_time=0.1255, lr=4.7961e-04, scale=1.0000
[2025-12-10 01:46:42] [INFO]  train | epoch 003:     892 /  1429  global_step=3750, loss=2.0990, nll_loss=2.0990, accuracy=0.5335, micro_step_time=0.0614, step_time=0.1255, lr=4.7906e-04, scale=1.0000
[2025-12-10 01:46:48] [INFO]  train | epoch 003:     942 /  1429  global_step=3800, loss=2.0645, nll_loss=2.0645, accuracy=0.5394, micro_step_time=0.0614, step_time=0.1255, lr=4.7851e-04, scale=1.0000
[2025-12-10 01:46:54] [INFO]  train | epoch 003:     992 /  1429  global_step=3850, loss=2.0634, nll_loss=2.0634, accuracy=0.5434, micro_step_time=0.0614, step_time=0.1254, lr=4.7795e-04, scale=1.0000
[2025-12-10 01:47:01] [INFO]  train | epoch 003:    1042 /  1429  global_step=3900, loss=2.2148, nll_loss=2.2148, accuracy=0.5125, micro_step_time=0.0615, step_time=0.1257, lr=4.7738e-04, scale=1.0000
[2025-12-10 01:47:07] [INFO]  train | epoch 003:    1092 /  1429  global_step=3950, loss=2.1120, nll_loss=2.1120, accuracy=0.5321, micro_step_time=0.0579, step_time=0.1182, lr=4.7681e-04, scale=1.0000
[2025-12-10 01:47:12] [INFO]  train | epoch 003:    1142 /  1429  global_step=4000, loss=2.1645, nll_loss=2.1645, accuracy=0.5213, micro_step_time=0.0562, step_time=0.1145, lr=4.7623e-04, scale=1.0000
[2025-12-10 01:47:18] [INFO]  train | epoch 003:    1192 /  1429  global_step=4050, loss=2.0910, nll_loss=2.0910, accuracy=0.5393, micro_step_time=0.0553, step_time=0.1125, lr=4.7564e-04, scale=1.0000
[2025-12-10 01:47:24] [INFO]  train | epoch 003:    1242 /  1429  global_step=4100, loss=2.0846, nll_loss=2.0846, accuracy=0.5390, micro_step_time=0.0553, step_time=0.1124, lr=4.7504e-04, scale=1.0000
[2025-12-10 01:47:28] [INFO]  train | epoch 003:    1292 /  1429  global_step=4150, loss=2.1777, nll_loss=2.1777, accuracy=0.5153, micro_step_time=0.0475, step_time=0.0967, lr=4.7444e-04, scale=1.0000
[2025-12-10 01:47:33] [INFO]  train | epoch 003:    1342 /  1429  global_step=4200, loss=2.2184, nll_loss=2.2184, accuracy=0.5129, micro_step_time=0.0432, step_time=0.0882, lr=4.7383e-04, scale=1.0000
[2025-12-10 01:47:37] [INFO]  train | epoch 003:    1392 /  1429  global_step=4250, loss=2.1511, nll_loss=2.1511, accuracy=0.5304, micro_step_time=0.0433, step_time=0.0883, lr=4.7322e-04, scale=1.0000
[2025-12-10 01:47:40] [INFO]  End of epoch 3
[2025-12-10 01:47:40] [INFO]  train | epoch 003 | loss 2.0234 | nll_loss 2.0234 | kd_loss 0.0000
[2025-12-10 01:47:40] [INFO]  Evaluating before saving model...
[2025-12-10 01:47:40] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 01:48:49] [INFO]  eval_results in run@1: {'exact_match': 1.8, 'rougeL': 18.2282}
[2025-12-10 01:49:55] [INFO]  eval_results in run@2: {'exact_match': 2.3, 'rougeL': 18.9321}
[2025-12-10 01:51:03] [INFO]  eval_results in run@3: {'exact_match': 1.9, 'rougeL': 17.895}
[2025-12-10 01:51:03] [INFO]  dev | {'loss': 3.355894, 'token_num': 75795, 'token_acc': 0.417006, 'top1_prob': 0.538123} | {'exact_match': 2.0, 'rougeL': 18.3518}
[2025-12-10 01:51:03] [INFO]  Saving tokenizer...
[2025-12-10 01:51:03] [INFO]  Saving model...
[2025-12-10 01:51:04] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch3_step4287_loss3.3559_rougel18.3518
[2025-12-10 01:51:04] [INFO]  Start iterations of epoch 4
[2025-12-10 01:51:05] [INFO]  train | epoch 004:      13 /  1429  global_step=4300, loss=1.9536, nll_loss=1.9536, accuracy=0.5623, micro_step_time=0.0436, step_time=0.0890, lr=4.7259e-04, scale=1.0000
[2025-12-10 01:51:09] [INFO]  train | epoch 004:      63 /  1429  global_step=4350, loss=1.4301, nll_loss=1.4301, accuracy=0.6495, micro_step_time=0.0433, step_time=0.0884, lr=4.7197e-04, scale=1.0000
[2025-12-10 01:51:14] [INFO]  train | epoch 004:     113 /  1429  global_step=4400, loss=1.4975, nll_loss=1.4975, accuracy=0.6327, micro_step_time=0.0433, step_time=0.0884, lr=4.7133e-04, scale=1.0000
[2025-12-10 01:51:18] [INFO]  train | epoch 004:     163 /  1429  global_step=4450, loss=1.4007, nll_loss=1.4007, accuracy=0.6485, micro_step_time=0.0433, step_time=0.0883, lr=4.7069e-04, scale=1.0000
[2025-12-10 01:51:22] [INFO]  train | epoch 004:     213 /  1429  global_step=4500, loss=1.5386, nll_loss=1.5386, accuracy=0.6164, micro_step_time=0.0433, step_time=0.0883, lr=4.7004e-04, scale=1.0000
[2025-12-10 01:51:28] [INFO]  train | epoch 004:     263 /  1429  global_step=4550, loss=1.5104, nll_loss=1.5104, accuracy=0.6276, micro_step_time=0.0522, step_time=0.1066, lr=4.6938e-04, scale=1.0000
[2025-12-10 01:51:34] [INFO]  train | epoch 004:     313 /  1429  global_step=4600, loss=1.4720, nll_loss=1.4720, accuracy=0.6324, micro_step_time=0.0583, step_time=0.1193, lr=4.6872e-04, scale=1.0000
[2025-12-10 01:51:40] [INFO]  train | epoch 004:     363 /  1429  global_step=4650, loss=1.4939, nll_loss=1.4939, accuracy=0.6291, micro_step_time=0.0582, step_time=0.1191, lr=4.6805e-04, scale=1.0000
[2025-12-10 01:51:46] [INFO]  train | epoch 004:     413 /  1429  global_step=4700, loss=1.5934, nll_loss=1.5934, accuracy=0.6095, micro_step_time=0.0582, step_time=0.1191, lr=4.6738e-04, scale=1.0000
[2025-12-10 01:51:52] [INFO]  train | epoch 004:     463 /  1429  global_step=4750, loss=1.5802, nll_loss=1.5802, accuracy=0.6108, micro_step_time=0.0583, step_time=0.1193, lr=4.6670e-04, scale=1.0000
[2025-12-10 01:51:58] [INFO]  train | epoch 004:     513 /  1429  global_step=4800, loss=1.5510, nll_loss=1.5510, accuracy=0.6198, micro_step_time=0.0583, step_time=0.1193, lr=4.6601e-04, scale=1.0000
[2025-12-10 01:52:03] [INFO]  train | epoch 004:     563 /  1429  global_step=4850, loss=1.5813, nll_loss=1.5813, accuracy=0.6100, micro_step_time=0.0581, step_time=0.1188, lr=4.6531e-04, scale=1.0000
[2025-12-10 01:52:09] [INFO]  train | epoch 004:     613 /  1429  global_step=4900, loss=1.5621, nll_loss=1.5621, accuracy=0.6230, micro_step_time=0.0581, step_time=0.1188, lr=4.6461e-04, scale=1.0000
[2025-12-10 01:52:15] [INFO]  train | epoch 004:     663 /  1429  global_step=4950, loss=1.5652, nll_loss=1.5652, accuracy=0.6176, micro_step_time=0.0582, step_time=0.1191, lr=4.6390e-04, scale=1.0000
[2025-12-10 01:52:21] [INFO]  train | epoch 004:     713 /  1429  global_step=5000, loss=1.6009, nll_loss=1.6009, accuracy=0.6105, micro_step_time=0.0582, step_time=0.1191, lr=4.6319e-04, scale=1.0000
[2025-12-10 01:52:27] [INFO]  train | epoch 004:     763 /  1429  global_step=5050, loss=1.5920, nll_loss=1.5920, accuracy=0.6080, micro_step_time=0.0582, step_time=0.1191, lr=4.6247e-04, scale=1.0000
[2025-12-10 01:52:33] [INFO]  train | epoch 004:     813 /  1429  global_step=5100, loss=1.7365, nll_loss=1.7365, accuracy=0.5850, micro_step_time=0.0582, step_time=0.1192, lr=4.6174e-04, scale=1.0000
[2025-12-10 01:52:39] [INFO]  train | epoch 004:     863 /  1429  global_step=5150, loss=1.6223, nll_loss=1.6223, accuracy=0.6069, micro_step_time=0.0582, step_time=0.1191, lr=4.6101e-04, scale=1.0000
[2025-12-10 01:52:45] [INFO]  train | epoch 004:     913 /  1429  global_step=5200, loss=1.6394, nll_loss=1.6394, accuracy=0.6025, micro_step_time=0.0583, step_time=0.1192, lr=4.6027e-04, scale=1.0000
[2025-12-10 01:52:51] [INFO]  train | epoch 004:     963 /  1429  global_step=5250, loss=1.6296, nll_loss=1.6296, accuracy=0.6009, micro_step_time=0.0582, step_time=0.1190, lr=4.5952e-04, scale=1.0000
[2025-12-10 01:52:57] [INFO]  train | epoch 004:    1013 /  1429  global_step=5300, loss=1.5956, nll_loss=1.5956, accuracy=0.6188, micro_step_time=0.0582, step_time=0.1191, lr=4.5877e-04, scale=1.0000
[2025-12-10 01:53:03] [INFO]  train | epoch 004:    1063 /  1429  global_step=5350, loss=1.6162, nll_loss=1.6162, accuracy=0.6059, micro_step_time=0.0581, step_time=0.1190, lr=4.5801e-04, scale=1.0000
[2025-12-10 01:53:09] [INFO]  train | epoch 004:    1113 /  1429  global_step=5400, loss=1.6300, nll_loss=1.6300, accuracy=0.6056, micro_step_time=0.0582, step_time=0.1190, lr=4.5724e-04, scale=1.0000
[2025-12-10 01:53:15] [INFO]  train | epoch 004:    1163 /  1429  global_step=5450, loss=1.6949, nll_loss=1.6949, accuracy=0.5926, micro_step_time=0.0581, step_time=0.1189, lr=4.5647e-04, scale=1.0000
[2025-12-10 01:53:21] [INFO]  train | epoch 004:    1213 /  1429  global_step=5500, loss=1.6684, nll_loss=1.6684, accuracy=0.5989, micro_step_time=0.0581, step_time=0.1189, lr=4.5569e-04, scale=1.0000
[2025-12-10 01:53:27] [INFO]  train | epoch 004:    1263 /  1429  global_step=5550, loss=1.7475, nll_loss=1.7475, accuracy=0.5810, micro_step_time=0.0582, step_time=0.1190, lr=4.5491e-04, scale=1.0000
[2025-12-10 01:53:33] [INFO]  train | epoch 004:    1313 /  1429  global_step=5600, loss=1.6727, nll_loss=1.6727, accuracy=0.6016, micro_step_time=0.0583, step_time=0.1192, lr=4.5412e-04, scale=1.0000
[2025-12-10 01:53:39] [INFO]  train | epoch 004:    1363 /  1429  global_step=5650, loss=1.7415, nll_loss=1.7415, accuracy=0.5839, micro_step_time=0.0582, step_time=0.1191, lr=4.5332e-04, scale=1.0000
[2025-12-10 01:53:45] [INFO]  train | epoch 004:    1413 /  1429  global_step=5700, loss=1.7142, nll_loss=1.7142, accuracy=0.5876, micro_step_time=0.0581, step_time=0.1190, lr=4.5252e-04, scale=1.0000
[2025-12-10 01:53:47] [INFO]  End of epoch 4
[2025-12-10 01:53:47] [INFO]  train | epoch 004 | loss 1.6316 | nll_loss 1.6316 | kd_loss 0.0000
[2025-12-10 01:53:47] [INFO]  Evaluating before saving model...
[2025-12-10 01:53:47] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 01:54:57] [INFO]  eval_results in run@1: {'exact_match': 2.1, 'rougeL': 19.9328}
[2025-12-10 01:56:04] [INFO]  eval_results in run@2: {'exact_match': 2.1, 'rougeL': 18.8517}
[2025-12-10 01:57:12] [INFO]  eval_results in run@3: {'exact_match': 2.0, 'rougeL': 19.0996}
[2025-12-10 01:57:12] [INFO]  dev | {'loss': 3.616703, 'token_num': 75795, 'token_acc': 0.409486, 'top1_prob': 0.573257} | {'exact_match': 2.0667, 'rougeL': 19.2947}
[2025-12-10 01:57:12] [INFO]  Saving tokenizer...
[2025-12-10 01:57:12] [INFO]  Saving model...
[2025-12-10 01:57:12] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch4_step5716_loss3.6167_rougel19.2947
[2025-12-10 01:57:12] [INFO]  Start iterations of epoch 5
[2025-12-10 01:57:16] [INFO]  train | epoch 005:      34 /  1429  global_step=5750, loss=1.2486, nll_loss=1.2486, accuracy=0.6851, micro_step_time=0.0583, step_time=0.1193, lr=4.5171e-04, scale=1.0000
[2025-12-10 01:57:22] [INFO]  train | epoch 005:      84 /  1429  global_step=5800, loss=1.0927, nll_loss=1.0927, accuracy=0.7102, micro_step_time=0.0584, step_time=0.1195, lr=4.5090e-04, scale=1.0000
[2025-12-10 01:57:28] [INFO]  train | epoch 005:     134 /  1429  global_step=5850, loss=1.0150, nll_loss=1.0150, accuracy=0.7328, micro_step_time=0.0582, step_time=0.1192, lr=4.5008e-04, scale=1.0000
[2025-12-10 01:57:34] [INFO]  train | epoch 005:     184 /  1429  global_step=5900, loss=1.0525, nll_loss=1.0525, accuracy=0.7218, micro_step_time=0.0583, step_time=0.1194, lr=4.4925e-04, scale=1.0000
[2025-12-10 01:57:40] [INFO]  train | epoch 005:     234 /  1429  global_step=5950, loss=1.1458, nll_loss=1.1458, accuracy=0.6995, micro_step_time=0.0582, step_time=0.1192, lr=4.4842e-04, scale=1.0000
[2025-12-10 01:57:46] [INFO]  train | epoch 005:     284 /  1429  global_step=6000, loss=1.0899, nll_loss=1.0899, accuracy=0.7094, micro_step_time=0.0582, step_time=0.1190, lr=4.4758e-04, scale=1.0000
[2025-12-10 01:57:52] [INFO]  train | epoch 005:     334 /  1429  global_step=6050, loss=1.0726, nll_loss=1.0726, accuracy=0.7167, micro_step_time=0.0582, step_time=0.1190, lr=4.4673e-04, scale=1.0000
[2025-12-10 01:57:58] [INFO]  train | epoch 005:     384 /  1429  global_step=6100, loss=1.1671, nll_loss=1.1671, accuracy=0.6952, micro_step_time=0.0583, step_time=0.1193, lr=4.4588e-04, scale=1.0000
[2025-12-10 01:58:04] [INFO]  train | epoch 005:     434 /  1429  global_step=6150, loss=1.1674, nll_loss=1.1674, accuracy=0.6943, micro_step_time=0.0582, step_time=0.1190, lr=4.4503e-04, scale=1.0000
[2025-12-10 01:58:10] [INFO]  train | epoch 005:     484 /  1429  global_step=6200, loss=1.2261, nll_loss=1.2261, accuracy=0.6789, micro_step_time=0.0582, step_time=0.1192, lr=4.4417e-04, scale=1.0000
[2025-12-10 01:58:16] [INFO]  train | epoch 005:     534 /  1429  global_step=6250, loss=1.1956, nll_loss=1.1956, accuracy=0.6858, micro_step_time=0.0582, step_time=0.1190, lr=4.4330e-04, scale=1.0000
[2025-12-10 01:58:22] [INFO]  train | epoch 005:     584 /  1429  global_step=6300, loss=1.1266, nll_loss=1.1266, accuracy=0.7023, micro_step_time=0.0582, step_time=0.1191, lr=4.4242e-04, scale=1.0000
[2025-12-10 01:58:28] [INFO]  train | epoch 005:     634 /  1429  global_step=6350, loss=1.2086, nll_loss=1.2086, accuracy=0.6842, micro_step_time=0.0582, step_time=0.1191, lr=4.4154e-04, scale=1.0000
[2025-12-10 01:58:34] [INFO]  train | epoch 005:     684 /  1429  global_step=6400, loss=1.2468, nll_loss=1.2468, accuracy=0.6771, micro_step_time=0.0582, step_time=0.1190, lr=4.4066e-04, scale=1.0000
[2025-12-10 01:58:40] [INFO]  train | epoch 005:     734 /  1429  global_step=6450, loss=1.2375, nll_loss=1.2375, accuracy=0.6777, micro_step_time=0.0582, step_time=0.1192, lr=4.3977e-04, scale=1.0000
[2025-12-10 01:58:45] [INFO]  train | epoch 005:     784 /  1429  global_step=6500, loss=1.3175, nll_loss=1.3175, accuracy=0.6576, micro_step_time=0.0582, step_time=0.1191, lr=4.3887e-04, scale=1.0000
[2025-12-10 01:58:51] [INFO]  train | epoch 005:     834 /  1429  global_step=6550, loss=1.2638, nll_loss=1.2638, accuracy=0.6718, micro_step_time=0.0583, step_time=0.1192, lr=4.3796e-04, scale=1.0000
[2025-12-10 01:58:57] [INFO]  train | epoch 005:     884 /  1429  global_step=6600, loss=1.2855, nll_loss=1.2855, accuracy=0.6699, micro_step_time=0.0582, step_time=0.1191, lr=4.3706e-04, scale=1.0000
[2025-12-10 01:59:03] [INFO]  train | epoch 005:     934 /  1429  global_step=6650, loss=1.2216, nll_loss=1.2216, accuracy=0.6827, micro_step_time=0.0582, step_time=0.1190, lr=4.3614e-04, scale=1.0000
[2025-12-10 01:59:08] [INFO]  train | epoch 005:     984 /  1429  global_step=6700, loss=1.2360, nll_loss=1.2360, accuracy=0.6767, micro_step_time=0.0465, step_time=0.0949, lr=4.3522e-04, scale=1.0000
[2025-12-10 01:59:13] [INFO]  train | epoch 005:    1034 /  1429  global_step=6750, loss=1.2675, nll_loss=1.2675, accuracy=0.6715, micro_step_time=0.0432, step_time=0.0883, lr=4.3430e-04, scale=1.0000
[2025-12-10 01:59:17] [INFO]  train | epoch 005:    1084 /  1429  global_step=6800, loss=1.2617, nll_loss=1.2617, accuracy=0.6721, micro_step_time=0.0433, step_time=0.0884, lr=4.3337e-04, scale=1.0000
[2025-12-10 01:59:21] [INFO]  train | epoch 005:    1134 /  1429  global_step=6850, loss=1.3180, nll_loss=1.3180, accuracy=0.6572, micro_step_time=0.0433, step_time=0.0883, lr=4.3243e-04, scale=1.0000
[2025-12-10 01:59:26] [INFO]  train | epoch 005:    1184 /  1429  global_step=6900, loss=1.2824, nll_loss=1.2824, accuracy=0.6635, micro_step_time=0.0432, step_time=0.0883, lr=4.3149e-04, scale=1.0000
[2025-12-10 01:59:30] [INFO]  train | epoch 005:    1234 /  1429  global_step=6950, loss=1.3029, nll_loss=1.3029, accuracy=0.6619, micro_step_time=0.0432, step_time=0.0882, lr=4.3054e-04, scale=1.0000
[2025-12-10 01:59:35] [INFO]  train | epoch 005:    1284 /  1429  global_step=7000, loss=1.3234, nll_loss=1.3234, accuracy=0.6597, micro_step_time=0.0433, step_time=0.0883, lr=4.2959e-04, scale=1.0000
[2025-12-10 01:59:39] [INFO]  train | epoch 005:    1334 /  1429  global_step=7050, loss=1.3115, nll_loss=1.3115, accuracy=0.6642, micro_step_time=0.0433, step_time=0.0884, lr=4.2863e-04, scale=1.0000
[2025-12-10 01:59:43] [INFO]  train | epoch 005:    1384 /  1429  global_step=7100, loss=1.3283, nll_loss=1.3283, accuracy=0.6600, micro_step_time=0.0433, step_time=0.0883, lr=4.2766e-04, scale=1.0000
[2025-12-10 01:59:47] [INFO]  End of epoch 5
[2025-12-10 01:59:47] [INFO]  train | epoch 005 | loss 1.1901 | nll_loss 1.1901 | kd_loss 0.0000
[2025-12-10 01:59:47] [INFO]  Evaluating before saving model...
[2025-12-10 01:59:47] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:00:55] [INFO]  eval_results in run@1: {'exact_match': 2.7, 'rougeL': 20.4748}
[2025-12-10 02:02:01] [INFO]  eval_results in run@2: {'exact_match': 2.4, 'rougeL': 20.2969}
[2025-12-10 02:03:05] [INFO]  eval_results in run@3: {'exact_match': 2.2, 'rougeL': 19.9179}
[2025-12-10 02:03:05] [INFO]  dev | {'loss': 4.006597, 'token_num': 75795, 'token_acc': 0.399261, 'top1_prob': 0.618669} | {'exact_match': 2.4333, 'rougeL': 20.2299}
[2025-12-10 02:03:05] [INFO]  Saving tokenizer...
[2025-12-10 02:03:05] [INFO]  Saving model...
[2025-12-10 02:03:06] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch5_step7145_loss4.0066_rougel20.2299
[2025-12-10 02:03:06] [INFO]  Start iterations of epoch 6
[2025-12-10 02:03:06] [INFO]  train | epoch 006:       5 /  1429  global_step=7150, loss=1.2732, nll_loss=1.2732, accuracy=0.6715, micro_step_time=0.0434, step_time=0.0887, lr=4.2669e-04, scale=1.0000
[2025-12-10 02:03:12] [INFO]  train | epoch 006:      55 /  1429  global_step=7200, loss=0.7835, nll_loss=0.7835, accuracy=0.7824, micro_step_time=0.0545, step_time=0.1116, lr=4.2572e-04, scale=1.0000
[2025-12-10 02:03:18] [INFO]  train | epoch 006:     105 /  1429  global_step=7250, loss=0.8073, nll_loss=0.8073, accuracy=0.7779, micro_step_time=0.0590, step_time=0.1207, lr=4.2474e-04, scale=1.0000
[2025-12-10 02:03:24] [INFO]  train | epoch 006:     155 /  1429  global_step=7300, loss=0.8722, nll_loss=0.8722, accuracy=0.7617, micro_step_time=0.0590, step_time=0.1207, lr=4.2375e-04, scale=1.0000
[2025-12-10 02:03:30] [INFO]  train | epoch 006:     205 /  1429  global_step=7350, loss=0.8473, nll_loss=0.8473, accuracy=0.7674, micro_step_time=0.0590, step_time=0.1207, lr=4.2276e-04, scale=1.0000
[2025-12-10 02:03:36] [INFO]  train | epoch 006:     255 /  1429  global_step=7400, loss=0.8279, nll_loss=0.8279, accuracy=0.7692, micro_step_time=0.0589, step_time=0.1206, lr=4.2177e-04, scale=1.0000
[2025-12-10 02:03:42] [INFO]  train | epoch 006:     305 /  1429  global_step=7450, loss=0.7983, nll_loss=0.7983, accuracy=0.7779, micro_step_time=0.0589, step_time=0.1205, lr=4.2077e-04, scale=1.0000
[2025-12-10 02:03:48] [INFO]  train | epoch 006:     355 /  1429  global_step=7500, loss=0.8447, nll_loss=0.8447, accuracy=0.7693, micro_step_time=0.0581, step_time=0.1190, lr=4.1976e-04, scale=1.0000
[2025-12-10 02:03:54] [INFO]  train | epoch 006:     405 /  1429  global_step=7550, loss=0.9350, nll_loss=0.9350, accuracy=0.7424, micro_step_time=0.0582, step_time=0.1191, lr=4.1875e-04, scale=1.0000
[2025-12-10 02:04:00] [INFO]  train | epoch 006:     455 /  1429  global_step=7600, loss=0.8762, nll_loss=0.8762, accuracy=0.7546, micro_step_time=0.0582, step_time=0.1191, lr=4.1773e-04, scale=1.0000
[2025-12-10 02:04:06] [INFO]  train | epoch 006:     505 /  1429  global_step=7650, loss=0.9278, nll_loss=0.9278, accuracy=0.7476, micro_step_time=0.0583, step_time=0.1193, lr=4.1671e-04, scale=1.0000
[2025-12-10 02:04:12] [INFO]  train | epoch 006:     555 /  1429  global_step=7700, loss=0.9006, nll_loss=0.9006, accuracy=0.7479, micro_step_time=0.0581, step_time=0.1190, lr=4.1569e-04, scale=1.0000
[2025-12-10 02:04:18] [INFO]  train | epoch 006:     605 /  1429  global_step=7750, loss=0.9285, nll_loss=0.9285, accuracy=0.7406, micro_step_time=0.0581, step_time=0.1190, lr=4.1466e-04, scale=1.0000
[2025-12-10 02:04:24] [INFO]  train | epoch 006:     655 /  1429  global_step=7800, loss=0.9261, nll_loss=0.9261, accuracy=0.7481, micro_step_time=0.0581, step_time=0.1189, lr=4.1362e-04, scale=1.0000
[2025-12-10 02:04:30] [INFO]  train | epoch 006:     705 /  1429  global_step=7850, loss=0.9304, nll_loss=0.9304, accuracy=0.7451, micro_step_time=0.0582, step_time=0.1190, lr=4.1258e-04, scale=1.0000
[2025-12-10 02:04:36] [INFO]  train | epoch 006:     755 /  1429  global_step=7900, loss=0.9423, nll_loss=0.9423, accuracy=0.7403, micro_step_time=0.0582, step_time=0.1190, lr=4.1153e-04, scale=1.0000
[2025-12-10 02:04:41] [INFO]  train | epoch 006:     805 /  1429  global_step=7950, loss=0.9086, nll_loss=0.9086, accuracy=0.7506, micro_step_time=0.0582, step_time=0.1190, lr=4.1048e-04, scale=1.0000
[2025-12-10 02:04:47] [INFO]  train | epoch 006:     855 /  1429  global_step=8000, loss=0.9443, nll_loss=0.9443, accuracy=0.7430, micro_step_time=0.0582, step_time=0.1190, lr=4.0942e-04, scale=1.0000
[2025-12-10 02:04:53] [INFO]  train | epoch 006:     905 /  1429  global_step=8050, loss=0.9242, nll_loss=0.9242, accuracy=0.7460, micro_step_time=0.0583, step_time=0.1192, lr=4.0836e-04, scale=1.0000
[2025-12-10 02:04:59] [INFO]  train | epoch 006:     955 /  1429  global_step=8100, loss=0.9359, nll_loss=0.9359, accuracy=0.7416, micro_step_time=0.0581, step_time=0.1189, lr=4.0730e-04, scale=1.0000
[2025-12-10 02:05:05] [INFO]  train | epoch 006:    1005 /  1429  global_step=8150, loss=0.9808, nll_loss=0.9808, accuracy=0.7331, micro_step_time=0.0581, step_time=0.1190, lr=4.0623e-04, scale=1.0000
[2025-12-10 02:05:11] [INFO]  train | epoch 006:    1055 /  1429  global_step=8200, loss=0.9941, nll_loss=0.9941, accuracy=0.7266, micro_step_time=0.0581, step_time=0.1190, lr=4.0515e-04, scale=1.0000
[2025-12-10 02:05:17] [INFO]  train | epoch 006:    1105 /  1429  global_step=8250, loss=1.0064, nll_loss=1.0064, accuracy=0.7248, micro_step_time=0.0582, step_time=0.1190, lr=4.0407e-04, scale=1.0000
[2025-12-10 02:05:23] [INFO]  train | epoch 006:    1155 /  1429  global_step=8300, loss=0.9478, nll_loss=0.9478, accuracy=0.7411, micro_step_time=0.0582, step_time=0.1190, lr=4.0299e-04, scale=1.0000
[2025-12-10 02:05:29] [INFO]  train | epoch 006:    1205 /  1429  global_step=8350, loss=0.9777, nll_loss=0.9777, accuracy=0.7324, micro_step_time=0.0581, step_time=0.1189, lr=4.0190e-04, scale=1.0000
[2025-12-10 02:05:35] [INFO]  train | epoch 006:    1255 /  1429  global_step=8400, loss=0.9558, nll_loss=0.9558, accuracy=0.7411, micro_step_time=0.0582, step_time=0.1191, lr=4.0081e-04, scale=1.0000
[2025-12-10 02:05:41] [INFO]  train | epoch 006:    1305 /  1429  global_step=8450, loss=0.9607, nll_loss=0.9607, accuracy=0.7385, micro_step_time=0.0582, step_time=0.1190, lr=3.9971e-04, scale=1.0000
[2025-12-10 02:05:47] [INFO]  train | epoch 006:    1355 /  1429  global_step=8500, loss=1.0376, nll_loss=1.0376, accuracy=0.7152, micro_step_time=0.0582, step_time=0.1190, lr=3.9861e-04, scale=1.0000
[2025-12-10 02:05:53] [INFO]  train | epoch 006:    1405 /  1429  global_step=8550, loss=1.0321, nll_loss=1.0321, accuracy=0.7215, micro_step_time=0.0582, step_time=0.1190, lr=3.9750e-04, scale=1.0000
[2025-12-10 02:05:56] [INFO]  End of epoch 6
[2025-12-10 02:05:56] [INFO]  train | epoch 006 | loss 0.9457 | nll_loss 0.9457 | kd_loss 0.0000
[2025-12-10 02:05:56] [INFO]  Evaluating before saving model...
[2025-12-10 02:05:56] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:07:06] [INFO]  eval_results in run@1: {'exact_match': 1.8, 'rougeL': 20.444}
[2025-12-10 02:08:14] [INFO]  eval_results in run@2: {'exact_match': 1.8, 'rougeL': 20.1959}
[2025-12-10 02:09:22] [INFO]  eval_results in run@3: {'exact_match': 1.6, 'rougeL': 20.2821}
[2025-12-10 02:09:22] [INFO]  dev | {'loss': 4.336117, 'token_num': 75795, 'token_acc': 0.396213, 'top1_prob': 0.649014} | {'exact_match': 1.7333, 'rougeL': 20.3073}
[2025-12-10 02:09:22] [INFO]  Saving tokenizer...
[2025-12-10 02:09:22] [INFO]  Saving model...
[2025-12-10 02:09:23] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch6_step8574_loss4.3361_rougel20.3073
[2025-12-10 02:09:23] [INFO]  Start iterations of epoch 7
[2025-12-10 02:09:25] [INFO]  train | epoch 007:      26 /  1429  global_step=8600, loss=0.7892, nll_loss=0.7892, accuracy=0.7827, micro_step_time=0.0555, step_time=0.1132, lr=3.9639e-04, scale=1.0000
[2025-12-10 02:09:31] [INFO]  train | epoch 007:      76 /  1429  global_step=8650, loss=0.5857, nll_loss=0.5857, accuracy=0.8302, micro_step_time=0.0560, step_time=0.1143, lr=3.9527e-04, scale=1.0000
[2025-12-10 02:09:37] [INFO]  train | epoch 007:     126 /  1429  global_step=8700, loss=0.6151, nll_loss=0.6151, accuracy=0.8268, micro_step_time=0.0581, step_time=0.1189, lr=3.9415e-04, scale=1.0000
[2025-12-10 02:09:43] [INFO]  train | epoch 007:     176 /  1429  global_step=8750, loss=0.6201, nll_loss=0.6201, accuracy=0.8187, micro_step_time=0.0581, step_time=0.1189, lr=3.9303e-04, scale=1.0000
[2025-12-10 02:09:49] [INFO]  train | epoch 007:     226 /  1429  global_step=8800, loss=0.6229, nll_loss=0.6229, accuracy=0.8214, micro_step_time=0.0581, step_time=0.1189, lr=3.9190e-04, scale=1.0000
[2025-12-10 02:09:55] [INFO]  train | epoch 007:     276 /  1429  global_step=8850, loss=0.6746, nll_loss=0.6746, accuracy=0.8079, micro_step_time=0.0581, step_time=0.1189, lr=3.9077e-04, scale=1.0000
[2025-12-10 02:10:01] [INFO]  train | epoch 007:     326 /  1429  global_step=8900, loss=0.6635, nll_loss=0.6635, accuracy=0.8111, micro_step_time=0.0582, step_time=0.1191, lr=3.8963e-04, scale=1.0000
[2025-12-10 02:10:07] [INFO]  train | epoch 007:     376 /  1429  global_step=8950, loss=0.6769, nll_loss=0.6769, accuracy=0.8062, micro_step_time=0.0581, step_time=0.1189, lr=3.8849e-04, scale=1.0000
[2025-12-10 02:10:13] [INFO]  train | epoch 007:     426 /  1429  global_step=9000, loss=0.6812, nll_loss=0.6812, accuracy=0.8036, micro_step_time=0.0581, step_time=0.1189, lr=3.8734e-04, scale=1.0000
[2025-12-10 02:10:19] [INFO]  train | epoch 007:     476 /  1429  global_step=9050, loss=0.6606, nll_loss=0.6606, accuracy=0.8138, micro_step_time=0.0583, step_time=0.1192, lr=3.8619e-04, scale=1.0000
[2025-12-10 02:10:25] [INFO]  train | epoch 007:     526 /  1429  global_step=9100, loss=0.6399, nll_loss=0.6399, accuracy=0.8128, micro_step_time=0.0582, step_time=0.1190, lr=3.8504e-04, scale=1.0000
[2025-12-10 02:10:31] [INFO]  train | epoch 007:     576 /  1429  global_step=9150, loss=0.6924, nll_loss=0.6924, accuracy=0.8005, micro_step_time=0.0582, step_time=0.1190, lr=3.8388e-04, scale=1.0000
[2025-12-10 02:10:37] [INFO]  train | epoch 007:     626 /  1429  global_step=9200, loss=0.6767, nll_loss=0.6767, accuracy=0.8050, micro_step_time=0.0582, step_time=0.1190, lr=3.8272e-04, scale=1.0000
[2025-12-10 02:10:43] [INFO]  train | epoch 007:     676 /  1429  global_step=9250, loss=0.6925, nll_loss=0.6925, accuracy=0.7998, micro_step_time=0.0581, step_time=0.1189, lr=3.8155e-04, scale=1.0000
[2025-12-10 02:10:49] [INFO]  train | epoch 007:     726 /  1429  global_step=9300, loss=0.6942, nll_loss=0.6942, accuracy=0.7989, micro_step_time=0.0582, step_time=0.1192, lr=3.8038e-04, scale=1.0000
[2025-12-10 02:10:54] [INFO]  train | epoch 007:     776 /  1429  global_step=9350, loss=0.7070, nll_loss=0.7070, accuracy=0.7936, micro_step_time=0.0581, step_time=0.1189, lr=3.7920e-04, scale=1.0000
[2025-12-10 02:11:00] [INFO]  train | epoch 007:     826 /  1429  global_step=9400, loss=0.7653, nll_loss=0.7653, accuracy=0.7807, micro_step_time=0.0583, step_time=0.1194, lr=3.7803e-04, scale=1.0000
[2025-12-10 02:11:06] [INFO]  train | epoch 007:     876 /  1429  global_step=9450, loss=0.7311, nll_loss=0.7311, accuracy=0.7898, micro_step_time=0.0516, step_time=0.1051, lr=3.7684e-04, scale=1.0000
[2025-12-10 02:11:11] [INFO]  train | epoch 007:     926 /  1429  global_step=9500, loss=0.6870, nll_loss=0.6870, accuracy=0.8004, micro_step_time=0.0526, step_time=0.1069, lr=3.7566e-04, scale=1.0000
[2025-12-10 02:11:16] [INFO]  train | epoch 007:     976 /  1429  global_step=9550, loss=0.7059, nll_loss=0.7059, accuracy=0.7973, micro_step_time=0.0525, step_time=0.1068, lr=3.7447e-04, scale=1.0000
[2025-12-10 02:11:22] [INFO]  train | epoch 007:    1026 /  1429  global_step=9600, loss=0.7123, nll_loss=0.7123, accuracy=0.7934, micro_step_time=0.0526, step_time=0.1069, lr=3.7328e-04, scale=1.0000
[2025-12-10 02:11:27] [INFO]  train | epoch 007:    1076 /  1429  global_step=9650, loss=0.7188, nll_loss=0.7188, accuracy=0.7946, micro_step_time=0.0526, step_time=0.1069, lr=3.7208e-04, scale=1.0000
[2025-12-10 02:11:32] [INFO]  train | epoch 007:    1126 /  1429  global_step=9700, loss=0.7540, nll_loss=0.7540, accuracy=0.7846, micro_step_time=0.0526, step_time=0.1069, lr=3.7088e-04, scale=1.0000
[2025-12-10 02:11:38] [INFO]  train | epoch 007:    1176 /  1429  global_step=9750, loss=0.7682, nll_loss=0.7682, accuracy=0.7816, micro_step_time=0.0526, step_time=0.1070, lr=3.6967e-04, scale=1.0000
[2025-12-10 02:11:44] [INFO]  train | epoch 007:    1226 /  1429  global_step=9800, loss=0.7672, nll_loss=0.7672, accuracy=0.7809, micro_step_time=0.0578, step_time=0.1182, lr=3.6847e-04, scale=1.0000
[2025-12-10 02:11:50] [INFO]  train | epoch 007:    1276 /  1429  global_step=9850, loss=0.7426, nll_loss=0.7426, accuracy=0.7862, micro_step_time=0.0581, step_time=0.1190, lr=3.6725e-04, scale=1.0000
[2025-12-10 02:11:56] [INFO]  train | epoch 007:    1326 /  1429  global_step=9900, loss=0.7410, nll_loss=0.7410, accuracy=0.7874, micro_step_time=0.0581, step_time=0.1189, lr=3.6604e-04, scale=1.0000
[2025-12-10 02:12:02] [INFO]  train | epoch 007:    1376 /  1429  global_step=9950, loss=0.7637, nll_loss=0.7637, accuracy=0.7824, micro_step_time=0.0583, step_time=0.1192, lr=3.6482e-04, scale=1.0000
[2025-12-10 02:12:08] [INFO]  train | epoch 007:    1426 /  1429  global_step=10000, loss=0.6958, nll_loss=0.6958, accuracy=0.7990, micro_step_time=0.0582, step_time=0.1191, lr=3.6360e-04, scale=1.0000
[2025-12-10 02:12:08] [INFO]  End of epoch 7
[2025-12-10 02:12:08] [INFO]  train | epoch 007 | loss 0.7084 | nll_loss 0.7084 | kd_loss 0.0000
[2025-12-10 02:12:08] [INFO]  Evaluating before saving model...
[2025-12-10 02:12:08] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:13:15] [INFO]  eval_results in run@1: {'exact_match': 2.3, 'rougeL': 21.5161}
[2025-12-10 02:14:20] [INFO]  eval_results in run@2: {'exact_match': 2.5, 'rougeL': 21.5108}
[2025-12-10 02:15:20] [INFO]  eval_results in run@3: {'exact_match': 2.2, 'rougeL': 21.1276}
[2025-12-10 02:15:20] [INFO]  dev | {'loss': 4.772452, 'token_num': 75795, 'token_acc': 0.391978, 'top1_prob': 0.689702} | {'exact_match': 2.3333, 'rougeL': 21.3848}
[2025-12-10 02:15:20] [INFO]  Saving tokenizer...
[2025-12-10 02:15:20] [INFO]  Saving model...
[2025-12-10 02:15:21] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch7_step10003_loss4.7725_rougel21.3848
[2025-12-10 02:15:21] [INFO]  Start iterations of epoch 8
[2025-12-10 02:15:26] [INFO]  train | epoch 008:      47 /  1429  global_step=10050, loss=0.4743, nll_loss=0.4743, accuracy=0.8624, micro_step_time=0.0581, step_time=0.1188, lr=3.6237e-04, scale=1.0000
[2025-12-10 02:15:32] [INFO]  train | epoch 008:      97 /  1429  global_step=10100, loss=0.4507, nll_loss=0.4507, accuracy=0.8671, micro_step_time=0.0582, step_time=0.1190, lr=3.6114e-04, scale=1.0000
[2025-12-10 02:15:38] [INFO]  train | epoch 008:     147 /  1429  global_step=10150, loss=0.4664, nll_loss=0.4664, accuracy=0.8642, micro_step_time=0.0590, step_time=0.1206, lr=3.5991e-04, scale=1.0000
[2025-12-10 02:15:45] [INFO]  train | epoch 008:     197 /  1429  global_step=10200, loss=0.4779, nll_loss=0.4779, accuracy=0.8583, micro_step_time=0.0614, step_time=0.1255, lr=3.5868e-04, scale=1.0000
[2025-12-10 02:15:51] [INFO]  train | epoch 008:     247 /  1429  global_step=10250, loss=0.4816, nll_loss=0.4816, accuracy=0.8556, micro_step_time=0.0613, step_time=0.1253, lr=3.5744e-04, scale=1.0000
[2025-12-10 02:15:57] [INFO]  train | epoch 008:     297 /  1429  global_step=10300, loss=0.5019, nll_loss=0.5019, accuracy=0.8539, micro_step_time=0.0612, step_time=0.1252, lr=3.5620e-04, scale=1.0000
[2025-12-10 02:16:03] [INFO]  train | epoch 008:     347 /  1429  global_step=10350, loss=0.5031, nll_loss=0.5031, accuracy=0.8515, micro_step_time=0.0613, step_time=0.1253, lr=3.5495e-04, scale=1.0000
[2025-12-10 02:16:10] [INFO]  train | epoch 008:     397 /  1429  global_step=10400, loss=0.4989, nll_loss=0.4989, accuracy=0.8517, micro_step_time=0.0613, step_time=0.1253, lr=3.5370e-04, scale=1.0000
[2025-12-10 02:16:16] [INFO]  train | epoch 008:     447 /  1429  global_step=10450, loss=0.5115, nll_loss=0.5115, accuracy=0.8490, micro_step_time=0.0611, step_time=0.1250, lr=3.5245e-04, scale=1.0000
[2025-12-10 02:16:22] [INFO]  train | epoch 008:     497 /  1429  global_step=10500, loss=0.5094, nll_loss=0.5094, accuracy=0.8505, micro_step_time=0.0613, step_time=0.1254, lr=3.5119e-04, scale=1.0000
[2025-12-10 02:16:28] [INFO]  train | epoch 008:     547 /  1429  global_step=10550, loss=0.5097, nll_loss=0.5097, accuracy=0.8478, micro_step_time=0.0613, step_time=0.1253, lr=3.4994e-04, scale=1.0000
[2025-12-10 02:16:35] [INFO]  train | epoch 008:     597 /  1429  global_step=10600, loss=0.5430, nll_loss=0.5430, accuracy=0.8404, micro_step_time=0.0613, step_time=0.1253, lr=3.4868e-04, scale=1.0000
[2025-12-10 02:16:41] [INFO]  train | epoch 008:     647 /  1429  global_step=10650, loss=0.4996, nll_loss=0.4996, accuracy=0.8525, micro_step_time=0.0613, step_time=0.1253, lr=3.4741e-04, scale=1.0000
[2025-12-10 02:16:47] [INFO]  train | epoch 008:     697 /  1429  global_step=10700, loss=0.5257, nll_loss=0.5257, accuracy=0.8429, micro_step_time=0.0613, step_time=0.1254, lr=3.4615e-04, scale=1.0000
[2025-12-10 02:16:54] [INFO]  train | epoch 008:     747 /  1429  global_step=10750, loss=0.5390, nll_loss=0.5390, accuracy=0.8422, micro_step_time=0.0613, step_time=0.1253, lr=3.4488e-04, scale=1.0000
[2025-12-10 02:17:00] [INFO]  train | epoch 008:     797 /  1429  global_step=10800, loss=0.5400, nll_loss=0.5400, accuracy=0.8399, micro_step_time=0.0613, step_time=0.1252, lr=3.4360e-04, scale=1.0000
[2025-12-10 02:17:06] [INFO]  train | epoch 008:     847 /  1429  global_step=10850, loss=0.5297, nll_loss=0.5297, accuracy=0.8403, micro_step_time=0.0613, step_time=0.1252, lr=3.4233e-04, scale=1.0000
[2025-12-10 02:17:12] [INFO]  train | epoch 008:     897 /  1429  global_step=10900, loss=0.5274, nll_loss=0.5274, accuracy=0.8436, micro_step_time=0.0586, step_time=0.1197, lr=3.4105e-04, scale=1.0000
[2025-12-10 02:17:18] [INFO]  train | epoch 008:     947 /  1429  global_step=10950, loss=0.5101, nll_loss=0.5101, accuracy=0.8482, micro_step_time=0.0590, step_time=0.1203, lr=3.3977e-04, scale=1.0000
[2025-12-10 02:17:24] [INFO]  train | epoch 008:     997 /  1429  global_step=11000, loss=0.5155, nll_loss=0.5155, accuracy=0.8440, micro_step_time=0.0612, step_time=0.1250, lr=3.3849e-04, scale=1.0000
[2025-12-10 02:17:31] [INFO]  train | epoch 008:    1047 /  1429  global_step=11050, loss=0.5470, nll_loss=0.5470, accuracy=0.8396, micro_step_time=0.0611, step_time=0.1249, lr=3.3720e-04, scale=1.0000
[2025-12-10 02:17:37] [INFO]  train | epoch 008:    1097 /  1429  global_step=11100, loss=0.5318, nll_loss=0.5318, accuracy=0.8390, micro_step_time=0.0612, step_time=0.1251, lr=3.3591e-04, scale=1.0000
[2025-12-10 02:17:43] [INFO]  train | epoch 008:    1147 /  1429  global_step=11150, loss=0.5904, nll_loss=0.5904, accuracy=0.8231, micro_step_time=0.0610, step_time=0.1246, lr=3.3462e-04, scale=1.0000
[2025-12-10 02:17:49] [INFO]  train | epoch 008:    1197 /  1429  global_step=11200, loss=0.5154, nll_loss=0.5154, accuracy=0.8438, micro_step_time=0.0610, step_time=0.1247, lr=3.3333e-04, scale=1.0000
[2025-12-10 02:17:55] [INFO]  train | epoch 008:    1247 /  1429  global_step=11250, loss=0.5680, nll_loss=0.5680, accuracy=0.8299, micro_step_time=0.0582, step_time=0.1191, lr=3.3203e-04, scale=1.0000
[2025-12-10 02:18:01] [INFO]  train | epoch 008:    1297 /  1429  global_step=11300, loss=0.5745, nll_loss=0.5745, accuracy=0.8315, micro_step_time=0.0576, step_time=0.1178, lr=3.3073e-04, scale=1.0000
[2025-12-10 02:18:07] [INFO]  train | epoch 008:    1347 /  1429  global_step=11350, loss=0.5579, nll_loss=0.5579, accuracy=0.8352, micro_step_time=0.0615, step_time=0.1258, lr=3.2943e-04, scale=1.0000
[2025-12-10 02:18:14] [INFO]  train | epoch 008:    1397 /  1429  global_step=11400, loss=0.6152, nll_loss=0.6152, accuracy=0.8182, micro_step_time=0.0615, step_time=0.1258, lr=3.2812e-04, scale=1.0000
[2025-12-10 02:18:18] [INFO]  End of epoch 8
[2025-12-10 02:18:18] [INFO]  train | epoch 008 | loss 0.5114 | nll_loss 0.5114 | kd_loss 0.0000
[2025-12-10 02:18:18] [INFO]  Evaluating before saving model...
[2025-12-10 02:18:18] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:19:27] [INFO]  eval_results in run@1: {'exact_match': 2.6, 'rougeL': 22.8886}
[2025-12-10 02:20:31] [INFO]  eval_results in run@2: {'exact_match': 2.6, 'rougeL': 22.6044}
[2025-12-10 02:21:37] [INFO]  eval_results in run@3: {'exact_match': 2.5, 'rougeL': 21.9537}
[2025-12-10 02:21:37] [INFO]  dev | {'loss': 5.030833, 'token_num': 75795, 'token_acc': 0.391134, 'top1_prob': 0.711736} | {'exact_match': 2.5667, 'rougeL': 22.4822}
[2025-12-10 02:21:37] [INFO]  Saving tokenizer...
[2025-12-10 02:21:37] [INFO]  Saving model...
[2025-12-10 02:21:38] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch8_step11432_loss5.0308_rougel22.4822
[2025-12-10 02:21:38] [INFO]  Start iterations of epoch 9
[2025-12-10 02:21:40] [INFO]  train | epoch 009:      18 /  1429  global_step=11450, loss=0.5044, nll_loss=0.5044, accuracy=0.8534, micro_step_time=0.0596, step_time=0.1217, lr=3.2682e-04, scale=1.0000
[2025-12-10 02:21:46] [INFO]  train | epoch 009:      68 /  1429  global_step=11500, loss=0.3004, nll_loss=0.3004, accuracy=0.9113, micro_step_time=0.0557, step_time=0.1132, lr=3.2551e-04, scale=1.0000
[2025-12-10 02:21:51] [INFO]  train | epoch 009:     118 /  1429  global_step=11550, loss=0.3447, nll_loss=0.3447, accuracy=0.8957, micro_step_time=0.0557, step_time=0.1133, lr=3.2420e-04, scale=1.0000
[2025-12-10 02:21:57] [INFO]  train | epoch 009:     168 /  1429  global_step=11600, loss=0.3697, nll_loss=0.3697, accuracy=0.8919, micro_step_time=0.0558, step_time=0.1134, lr=3.2289e-04, scale=1.0000
[2025-12-10 02:22:03] [INFO]  train | epoch 009:     218 /  1429  global_step=11650, loss=0.3354, nll_loss=0.3354, accuracy=0.8992, micro_step_time=0.0573, step_time=0.1166, lr=3.2157e-04, scale=1.0000
[2025-12-10 02:22:09] [INFO]  train | epoch 009:     268 /  1429  global_step=11700, loss=0.4144, nll_loss=0.4144, accuracy=0.8777, micro_step_time=0.0612, step_time=0.1252, lr=3.2025e-04, scale=1.0000
[2025-12-10 02:22:15] [INFO]  train | epoch 009:     318 /  1429  global_step=11750, loss=0.3567, nll_loss=0.3567, accuracy=0.8925, micro_step_time=0.0576, step_time=0.1177, lr=3.1893e-04, scale=1.0000
[2025-12-10 02:22:21] [INFO]  train | epoch 009:     368 /  1429  global_step=11800, loss=0.3624, nll_loss=0.3624, accuracy=0.8913, micro_step_time=0.0612, step_time=0.1250, lr=3.1761e-04, scale=1.0000
[2025-12-10 02:22:27] [INFO]  train | epoch 009:     418 /  1429  global_step=11850, loss=0.3573, nll_loss=0.3573, accuracy=0.8956, micro_step_time=0.0610, step_time=0.1247, lr=3.1629e-04, scale=1.0000
[2025-12-10 02:22:34] [INFO]  train | epoch 009:     468 /  1429  global_step=11900, loss=0.3903, nll_loss=0.3903, accuracy=0.8843, micro_step_time=0.0614, step_time=0.1254, lr=3.1496e-04, scale=1.0000
[2025-12-10 02:22:40] [INFO]  train | epoch 009:     518 /  1429  global_step=11950, loss=0.3722, nll_loss=0.3722, accuracy=0.8878, micro_step_time=0.0610, step_time=0.1246, lr=3.1364e-04, scale=1.0000
[2025-12-10 02:22:46] [INFO]  train | epoch 009:     568 /  1429  global_step=12000, loss=0.4325, nll_loss=0.4325, accuracy=0.8713, micro_step_time=0.0608, step_time=0.1243, lr=3.1231e-04, scale=1.0000
[2025-12-10 02:22:52] [INFO]  train | epoch 009:     618 /  1429  global_step=12050, loss=0.3860, nll_loss=0.3860, accuracy=0.8838, micro_step_time=0.0605, step_time=0.1238, lr=3.1097e-04, scale=1.0000
[2025-12-10 02:22:59] [INFO]  train | epoch 009:     668 /  1429  global_step=12100, loss=0.3844, nll_loss=0.3844, accuracy=0.8837, micro_step_time=0.0607, step_time=0.1241, lr=3.0964e-04, scale=1.0000
[2025-12-10 02:23:05] [INFO]  train | epoch 009:     718 /  1429  global_step=12150, loss=0.3771, nll_loss=0.3771, accuracy=0.8867, micro_step_time=0.0611, step_time=0.1249, lr=3.0831e-04, scale=1.0000
[2025-12-10 02:23:11] [INFO]  train | epoch 009:     768 /  1429  global_step=12200, loss=0.4017, nll_loss=0.4017, accuracy=0.8790, micro_step_time=0.0612, step_time=0.1251, lr=3.0697e-04, scale=1.0000
[2025-12-10 02:23:17] [INFO]  train | epoch 009:     818 /  1429  global_step=12250, loss=0.3855, nll_loss=0.3855, accuracy=0.8835, micro_step_time=0.0607, step_time=0.1241, lr=3.0563e-04, scale=1.0000
[2025-12-10 02:23:23] [INFO]  train | epoch 009:     868 /  1429  global_step=12300, loss=0.3872, nll_loss=0.3872, accuracy=0.8856, micro_step_time=0.0605, step_time=0.1238, lr=3.0429e-04, scale=1.0000
[2025-12-10 02:23:30] [INFO]  train | epoch 009:     918 /  1429  global_step=12350, loss=0.3751, nll_loss=0.3751, accuracy=0.8886, micro_step_time=0.0607, step_time=0.1241, lr=3.0295e-04, scale=1.0000
[2025-12-10 02:23:36] [INFO]  train | epoch 009:     968 /  1429  global_step=12400, loss=0.3929, nll_loss=0.3929, accuracy=0.8782, micro_step_time=0.0599, step_time=0.1225, lr=3.0161e-04, scale=1.0000
[2025-12-10 02:23:42] [INFO]  train | epoch 009:    1018 /  1429  global_step=12450, loss=0.4125, nll_loss=0.4125, accuracy=0.8758, micro_step_time=0.0601, step_time=0.1229, lr=3.0026e-04, scale=1.0000
[2025-12-10 02:23:48] [INFO]  train | epoch 009:    1068 /  1429  global_step=12500, loss=0.4045, nll_loss=0.4045, accuracy=0.8786, micro_step_time=0.0604, step_time=0.1235, lr=2.9891e-04, scale=1.0000
[2025-12-10 02:23:54] [INFO]  train | epoch 009:    1118 /  1429  global_step=12550, loss=0.4066, nll_loss=0.4066, accuracy=0.8768, micro_step_time=0.0611, step_time=0.1250, lr=2.9757e-04, scale=1.0000
[2025-12-10 02:24:01] [INFO]  train | epoch 009:    1168 /  1429  global_step=12600, loss=0.4158, nll_loss=0.4158, accuracy=0.8731, micro_step_time=0.0609, step_time=0.1245, lr=2.9622e-04, scale=1.0000
[2025-12-10 02:24:07] [INFO]  train | epoch 009:    1218 /  1429  global_step=12650, loss=0.4054, nll_loss=0.4054, accuracy=0.8776, micro_step_time=0.0602, step_time=0.1230, lr=2.9487e-04, scale=1.0000
[2025-12-10 02:24:13] [INFO]  train | epoch 009:    1268 /  1429  global_step=12700, loss=0.4130, nll_loss=0.4130, accuracy=0.8757, micro_step_time=0.0610, step_time=0.1246, lr=2.9351e-04, scale=1.0000
[2025-12-10 02:24:19] [INFO]  train | epoch 009:    1318 /  1429  global_step=12750, loss=0.4135, nll_loss=0.4135, accuracy=0.8773, micro_step_time=0.0611, step_time=0.1249, lr=2.9216e-04, scale=1.0000
[2025-12-10 02:24:25] [INFO]  train | epoch 009:    1368 /  1429  global_step=12800, loss=0.4072, nll_loss=0.4072, accuracy=0.8757, micro_step_time=0.0610, step_time=0.1246, lr=2.9081e-04, scale=1.0000
[2025-12-10 02:24:32] [INFO]  train | epoch 009:    1418 /  1429  global_step=12850, loss=0.4106, nll_loss=0.4106, accuracy=0.8773, micro_step_time=0.0608, step_time=0.1243, lr=2.8945e-04, scale=1.0000
[2025-12-10 02:24:33] [INFO]  End of epoch 9
[2025-12-10 02:24:33] [INFO]  train | epoch 009 | loss 0.3961 | nll_loss 0.3961 | kd_loss 0.0000
[2025-12-10 02:24:33] [INFO]  Evaluating before saving model...
[2025-12-10 02:24:33] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:25:39] [INFO]  eval_results in run@1: {'exact_match': 2.5, 'rougeL': 21.8755}
[2025-12-10 02:26:46] [INFO]  eval_results in run@2: {'exact_match': 2.2, 'rougeL': 21.9399}
[2025-12-10 02:27:51] [INFO]  eval_results in run@3: {'exact_match': 2.7, 'rougeL': 22.1349}
[2025-12-10 02:27:51] [INFO]  dev | {'loss': 5.334389, 'token_num': 75795, 'token_acc': 0.387004, 'top1_prob': 0.729177} | {'exact_match': 2.4667, 'rougeL': 21.9834}
[2025-12-10 02:27:51] [INFO]  Saving tokenizer...
[2025-12-10 02:27:52] [INFO]  Saving model...
[2025-12-10 02:27:52] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch9_step12861_loss5.3344_rougel21.9834
[2025-12-10 02:27:52] [INFO]  Start iterations of epoch 10
[2025-12-10 02:27:57] [INFO]  train | epoch 010:      39 /  1429  global_step=12900, loss=0.2724, nll_loss=0.2724, accuracy=0.9218, micro_step_time=0.0613, step_time=0.1253, lr=2.8809e-04, scale=1.0000
[2025-12-10 02:28:03] [INFO]  train | epoch 010:      89 /  1429  global_step=12950, loss=0.2609, nll_loss=0.2609, accuracy=0.9204, micro_step_time=0.0605, step_time=0.1237, lr=2.8673e-04, scale=1.0000
[2025-12-10 02:28:09] [INFO]  train | epoch 010:     139 /  1429  global_step=13000, loss=0.2727, nll_loss=0.2727, accuracy=0.9190, micro_step_time=0.0602, step_time=0.1231, lr=2.8537e-04, scale=1.0000
[2025-12-10 02:28:15] [INFO]  train | epoch 010:     189 /  1429  global_step=13050, loss=0.2566, nll_loss=0.2566, accuracy=0.9217, micro_step_time=0.0603, step_time=0.1233, lr=2.8401e-04, scale=1.0000
[2025-12-10 02:28:22] [INFO]  train | epoch 010:     239 /  1429  global_step=13100, loss=0.2623, nll_loss=0.2623, accuracy=0.9208, micro_step_time=0.0604, step_time=0.1235, lr=2.8265e-04, scale=1.0000
[2025-12-10 02:28:28] [INFO]  train | epoch 010:     289 /  1429  global_step=13150, loss=0.2640, nll_loss=0.2640, accuracy=0.9187, micro_step_time=0.0604, step_time=0.1235, lr=2.8129e-04, scale=1.0000
[2025-12-10 02:28:34] [INFO]  train | epoch 010:     339 /  1429  global_step=13200, loss=0.2590, nll_loss=0.2590, accuracy=0.9236, micro_step_time=0.0609, step_time=0.1245, lr=2.7993e-04, scale=1.0000
[2025-12-10 02:28:40] [INFO]  train | epoch 010:     389 /  1429  global_step=13250, loss=0.2843, nll_loss=0.2843, accuracy=0.9148, micro_step_time=0.0609, step_time=0.1245, lr=2.7856e-04, scale=1.0000
[2025-12-10 02:28:46] [INFO]  train | epoch 010:     439 /  1429  global_step=13300, loss=0.2893, nll_loss=0.2893, accuracy=0.9140, micro_step_time=0.0604, step_time=0.1234, lr=2.7720e-04, scale=1.0000
[2025-12-10 02:28:53] [INFO]  train | epoch 010:     489 /  1429  global_step=13350, loss=0.2924, nll_loss=0.2924, accuracy=0.9115, micro_step_time=0.0603, step_time=0.1234, lr=2.7583e-04, scale=1.0000
[2025-12-10 02:28:59] [INFO]  train | epoch 010:     539 /  1429  global_step=13400, loss=0.2695, nll_loss=0.2695, accuracy=0.9166, micro_step_time=0.0601, step_time=0.1229, lr=2.7446e-04, scale=1.0000
[2025-12-10 02:29:05] [INFO]  train | epoch 010:     589 /  1429  global_step=13450, loss=0.2703, nll_loss=0.2703, accuracy=0.9193, micro_step_time=0.0604, step_time=0.1235, lr=2.7310e-04, scale=1.0000
[2025-12-10 02:29:11] [INFO]  train | epoch 010:     639 /  1429  global_step=13500, loss=0.2866, nll_loss=0.2866, accuracy=0.9136, micro_step_time=0.0609, step_time=0.1244, lr=2.7173e-04, scale=1.0000
[2025-12-10 02:29:17] [INFO]  train | epoch 010:     689 /  1429  global_step=13550, loss=0.2760, nll_loss=0.2760, accuracy=0.9157, micro_step_time=0.0608, step_time=0.1244, lr=2.7036e-04, scale=1.0000
[2025-12-10 02:29:24] [INFO]  train | epoch 010:     739 /  1429  global_step=13600, loss=0.2778, nll_loss=0.2778, accuracy=0.9165, micro_step_time=0.0608, step_time=0.1242, lr=2.6899e-04, scale=1.0000
[2025-12-10 02:29:30] [INFO]  train | epoch 010:     789 /  1429  global_step=13650, loss=0.2964, nll_loss=0.2964, accuracy=0.9087, micro_step_time=0.0608, step_time=0.1243, lr=2.6762e-04, scale=1.0000
[2025-12-10 02:29:36] [INFO]  train | epoch 010:     839 /  1429  global_step=13700, loss=0.2949, nll_loss=0.2949, accuracy=0.9093, micro_step_time=0.0606, step_time=0.1238, lr=2.6625e-04, scale=1.0000
[2025-12-10 02:29:42] [INFO]  train | epoch 010:     889 /  1429  global_step=13750, loss=0.2732, nll_loss=0.2732, accuracy=0.9179, micro_step_time=0.0609, step_time=0.1244, lr=2.6488e-04, scale=1.0000
[2025-12-10 02:29:48] [INFO]  train | epoch 010:     939 /  1429  global_step=13800, loss=0.2783, nll_loss=0.2783, accuracy=0.9134, micro_step_time=0.0609, step_time=0.1246, lr=2.6351e-04, scale=1.0000
[2025-12-10 02:29:55] [INFO]  train | epoch 010:     989 /  1429  global_step=13850, loss=0.2829, nll_loss=0.2829, accuracy=0.9150, micro_step_time=0.0605, step_time=0.1236, lr=2.6213e-04, scale=1.0000
[2025-12-10 02:30:01] [INFO]  train | epoch 010:    1039 /  1429  global_step=13900, loss=0.2881, nll_loss=0.2881, accuracy=0.9116, micro_step_time=0.0604, step_time=0.1235, lr=2.6076e-04, scale=1.0000
[2025-12-10 02:30:07] [INFO]  train | epoch 010:    1089 /  1429  global_step=13950, loss=0.3022, nll_loss=0.3022, accuracy=0.9074, micro_step_time=0.0603, step_time=0.1233, lr=2.5939e-04, scale=1.0000
[2025-12-10 02:30:13] [INFO]  train | epoch 010:    1139 /  1429  global_step=14000, loss=0.3024, nll_loss=0.3024, accuracy=0.9071, micro_step_time=0.0606, step_time=0.1238, lr=2.5802e-04, scale=1.0000
[2025-12-10 02:30:19] [INFO]  train | epoch 010:    1189 /  1429  global_step=14050, loss=0.3002, nll_loss=0.3002, accuracy=0.9074, micro_step_time=0.0607, step_time=0.1242, lr=2.5664e-04, scale=1.0000
[2025-12-10 02:30:26] [INFO]  train | epoch 010:    1239 /  1429  global_step=14100, loss=0.2923, nll_loss=0.2923, accuracy=0.9110, micro_step_time=0.0609, step_time=0.1246, lr=2.5527e-04, scale=1.0000
[2025-12-10 02:30:32] [INFO]  train | epoch 010:    1289 /  1429  global_step=14150, loss=0.3058, nll_loss=0.3058, accuracy=0.9079, micro_step_time=0.0609, step_time=0.1245, lr=2.5390e-04, scale=1.0000
[2025-12-10 02:30:38] [INFO]  train | epoch 010:    1339 /  1429  global_step=14200, loss=0.2864, nll_loss=0.2864, accuracy=0.9130, micro_step_time=0.0609, step_time=0.1244, lr=2.5252e-04, scale=1.0000
[2025-12-10 02:30:44] [INFO]  train | epoch 010:    1389 /  1429  global_step=14250, loss=0.3020, nll_loss=0.3020, accuracy=0.9075, micro_step_time=0.0609, step_time=0.1244, lr=2.5115e-04, scale=1.0000
[2025-12-10 02:30:49] [INFO]  End of epoch 10
[2025-12-10 02:30:49] [INFO]  train | epoch 010 | loss 0.2764 | nll_loss 0.2764 | kd_loss 0.0000
[2025-12-10 02:30:49] [INFO]  Evaluating before saving model...
[2025-12-10 02:30:49] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:31:59] [INFO]  eval_results in run@1: {'exact_match': 2.8, 'rougeL': 23.1346}
[2025-12-10 02:33:05] [INFO]  eval_results in run@2: {'exact_match': 2.6, 'rougeL': 22.6707}
[2025-12-10 02:34:11] [INFO]  eval_results in run@3: {'exact_match': 2.9, 'rougeL': 23.2838}
[2025-12-10 02:34:11] [INFO]  dev | {'loss': 5.570183, 'token_num': 75795, 'token_acc': 0.38897, 'top1_prob': 0.750445} | {'exact_match': 2.7667, 'rougeL': 23.0297}
[2025-12-10 02:34:11] [INFO]  Saving tokenizer...
[2025-12-10 02:34:11] [INFO]  Saving model...
[2025-12-10 02:34:11] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch10_step14290_loss5.5702_rougel23.0297
[2025-12-10 02:34:11] [INFO]  Start iterations of epoch 11
[2025-12-10 02:34:12] [INFO]  train | epoch 011:      10 /  1429  global_step=14300, loss=0.2829, nll_loss=0.2829, accuracy=0.9151, micro_step_time=0.0606, step_time=0.1239, lr=2.4978e-04, scale=1.0000
[2025-12-10 02:34:19] [INFO]  train | epoch 011:      60 /  1429  global_step=14350, loss=0.1961, nll_loss=0.1961, accuracy=0.9419, micro_step_time=0.0610, step_time=0.1248, lr=2.4840e-04, scale=1.0000
[2025-12-10 02:34:25] [INFO]  train | epoch 011:     110 /  1429  global_step=14400, loss=0.1897, nll_loss=0.1897, accuracy=0.9443, micro_step_time=0.0609, step_time=0.1245, lr=2.4703e-04, scale=1.0000
[2025-12-10 02:34:31] [INFO]  train | epoch 011:     160 /  1429  global_step=14450, loss=0.1910, nll_loss=0.1910, accuracy=0.9432, micro_step_time=0.0609, step_time=0.1246, lr=2.4565e-04, scale=1.0000
[2025-12-10 02:34:37] [INFO]  train | epoch 011:     210 /  1429  global_step=14500, loss=0.1833, nll_loss=0.1833, accuracy=0.9454, micro_step_time=0.0609, step_time=0.1246, lr=2.4428e-04, scale=1.0000
[2025-12-10 02:34:44] [INFO]  train | epoch 011:     260 /  1429  global_step=14550, loss=0.1916, nll_loss=0.1916, accuracy=0.9420, micro_step_time=0.0610, step_time=0.1246, lr=2.4291e-04, scale=1.0000
[2025-12-10 02:34:50] [INFO]  train | epoch 011:     310 /  1429  global_step=14600, loss=0.2131, nll_loss=0.2131, accuracy=0.9352, micro_step_time=0.0610, step_time=0.1247, lr=2.4153e-04, scale=1.0000
[2025-12-10 02:34:56] [INFO]  train | epoch 011:     360 /  1429  global_step=14650, loss=0.1833, nll_loss=0.1833, accuracy=0.9454, micro_step_time=0.0609, step_time=0.1246, lr=2.4016e-04, scale=1.0000
[2025-12-10 02:35:02] [INFO]  train | epoch 011:     410 /  1429  global_step=14700, loss=0.2091, nll_loss=0.2091, accuracy=0.9376, micro_step_time=0.0610, step_time=0.1247, lr=2.3879e-04, scale=1.0000
[2025-12-10 02:35:09] [INFO]  train | epoch 011:     460 /  1429  global_step=14750, loss=0.2104, nll_loss=0.2104, accuracy=0.9370, micro_step_time=0.0609, step_time=0.1246, lr=2.3742e-04, scale=1.0000
[2025-12-10 02:35:15] [INFO]  train | epoch 011:     510 /  1429  global_step=14800, loss=0.1897, nll_loss=0.1897, accuracy=0.9422, micro_step_time=0.0610, step_time=0.1246, lr=2.3604e-04, scale=1.0000
[2025-12-10 02:35:21] [INFO]  train | epoch 011:     560 /  1429  global_step=14850, loss=0.2070, nll_loss=0.2070, accuracy=0.9382, micro_step_time=0.0609, step_time=0.1246, lr=2.3467e-04, scale=1.0000
[2025-12-10 02:35:27] [INFO]  train | epoch 011:     610 /  1429  global_step=14900, loss=0.2147, nll_loss=0.2147, accuracy=0.9357, micro_step_time=0.0610, step_time=0.1247, lr=2.3330e-04, scale=1.0000
[2025-12-10 02:35:33] [INFO]  train | epoch 011:     660 /  1429  global_step=14950, loss=0.2098, nll_loss=0.2098, accuracy=0.9371, micro_step_time=0.0609, step_time=0.1245, lr=2.3193e-04, scale=1.0000
[2025-12-10 02:35:40] [INFO]  train | epoch 011:     710 /  1429  global_step=15000, loss=0.2005, nll_loss=0.2005, accuracy=0.9398, micro_step_time=0.0610, step_time=0.1247, lr=2.3056e-04, scale=1.0000
[2025-12-10 02:35:46] [INFO]  train | epoch 011:     760 /  1429  global_step=15050, loss=0.2003, nll_loss=0.2003, accuracy=0.9400, micro_step_time=0.0610, step_time=0.1246, lr=2.2919e-04, scale=1.0000
[2025-12-10 02:35:52] [INFO]  train | epoch 011:     810 /  1429  global_step=15100, loss=0.2203, nll_loss=0.2203, accuracy=0.9320, micro_step_time=0.0610, step_time=0.1246, lr=2.2782e-04, scale=1.0000
[2025-12-10 02:35:58] [INFO]  train | epoch 011:     860 /  1429  global_step=15150, loss=0.2115, nll_loss=0.2115, accuracy=0.9364, micro_step_time=0.0610, step_time=0.1246, lr=2.2646e-04, scale=1.0000
[2025-12-10 02:36:05] [INFO]  train | epoch 011:     910 /  1429  global_step=15200, loss=0.2109, nll_loss=0.2109, accuracy=0.9358, micro_step_time=0.0610, step_time=0.1247, lr=2.2509e-04, scale=1.0000
[2025-12-10 02:36:11] [INFO]  train | epoch 011:     960 /  1429  global_step=15250, loss=0.2050, nll_loss=0.2050, accuracy=0.9365, micro_step_time=0.0610, step_time=0.1247, lr=2.2372e-04, scale=1.0000
[2025-12-10 02:36:17] [INFO]  train | epoch 011:    1010 /  1429  global_step=15300, loss=0.2177, nll_loss=0.2177, accuracy=0.9326, micro_step_time=0.0609, step_time=0.1245, lr=2.2236e-04, scale=1.0000
[2025-12-10 02:36:23] [INFO]  train | epoch 011:    1060 /  1429  global_step=15350, loss=0.2114, nll_loss=0.2114, accuracy=0.9356, micro_step_time=0.0610, step_time=0.1246, lr=2.2099e-04, scale=1.0000
[2025-12-10 02:36:30] [INFO]  train | epoch 011:    1110 /  1429  global_step=15400, loss=0.2193, nll_loss=0.2193, accuracy=0.9305, micro_step_time=0.0609, step_time=0.1245, lr=2.1963e-04, scale=1.0000
[2025-12-10 02:36:36] [INFO]  train | epoch 011:    1160 /  1429  global_step=15450, loss=0.2116, nll_loss=0.2116, accuracy=0.9356, micro_step_time=0.0610, step_time=0.1247, lr=2.1827e-04, scale=1.0000
[2025-12-10 02:36:42] [INFO]  train | epoch 011:    1210 /  1429  global_step=15500, loss=0.2190, nll_loss=0.2190, accuracy=0.9331, micro_step_time=0.0610, step_time=0.1247, lr=2.1690e-04, scale=1.0000
[2025-12-10 02:36:48] [INFO]  train | epoch 011:    1260 /  1429  global_step=15550, loss=0.2177, nll_loss=0.2177, accuracy=0.9329, micro_step_time=0.0610, step_time=0.1246, lr=2.1554e-04, scale=1.0000
[2025-12-10 02:36:55] [INFO]  train | epoch 011:    1310 /  1429  global_step=15600, loss=0.2076, nll_loss=0.2076, accuracy=0.9363, micro_step_time=0.0610, step_time=0.1246, lr=2.1418e-04, scale=1.0000
[2025-12-10 02:37:01] [INFO]  train | epoch 011:    1360 /  1429  global_step=15650, loss=0.2095, nll_loss=0.2095, accuracy=0.9343, micro_step_time=0.0610, step_time=0.1247, lr=2.1282e-04, scale=1.0000
[2025-12-10 02:37:07] [INFO]  train | epoch 011:    1410 /  1429  global_step=15700, loss=0.2114, nll_loss=0.2114, accuracy=0.9344, micro_step_time=0.0612, step_time=0.1250, lr=2.1146e-04, scale=1.0000
[2025-12-10 02:37:09] [INFO]  End of epoch 11
[2025-12-10 02:37:09] [INFO]  train | epoch 011 | loss 0.2115 | nll_loss 0.2115 | kd_loss 0.0000
[2025-12-10 02:37:09] [INFO]  Evaluating before saving model...
[2025-12-10 02:37:09] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:38:16] [INFO]  eval_results in run@1: {'exact_match': 2.9, 'rougeL': 22.8618}
[2025-12-10 02:39:18] [INFO]  eval_results in run@2: {'exact_match': 2.7, 'rougeL': 23.5105}
[2025-12-10 02:40:22] [INFO]  eval_results in run@3: {'exact_match': 2.4, 'rougeL': 23.2072}
[2025-12-10 02:40:22] [INFO]  dev | {'loss': 5.753414, 'token_num': 75795, 'token_acc': 0.388904, 'top1_prob': 0.76385} | {'exact_match': 2.6667, 'rougeL': 23.1932}
[2025-12-10 02:40:22] [INFO]  Saving tokenizer...
[2025-12-10 02:40:22] [INFO]  Saving model...
[2025-12-10 02:40:22] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch11_step15719_loss5.7534_rougel23.1932
[2025-12-10 02:40:22] [INFO]  Start iterations of epoch 12
[2025-12-10 02:40:26] [INFO]  train | epoch 012:      31 /  1429  global_step=15750, loss=0.1771, nll_loss=0.1771, accuracy=0.9443, micro_step_time=0.0612, step_time=0.1251, lr=2.1011e-04, scale=1.0000
[2025-12-10 02:40:32] [INFO]  train | epoch 012:      81 /  1429  global_step=15800, loss=0.1389, nll_loss=0.1389, accuracy=0.9587, micro_step_time=0.0613, step_time=0.1253, lr=2.0875e-04, scale=1.0000
[2025-12-10 02:40:38] [INFO]  train | epoch 012:     131 /  1429  global_step=15850, loss=0.1408, nll_loss=0.1408, accuracy=0.9582, micro_step_time=0.0612, step_time=0.1252, lr=2.0740e-04, scale=1.0000
[2025-12-10 02:40:45] [INFO]  train | epoch 012:     181 /  1429  global_step=15900, loss=0.1505, nll_loss=0.1505, accuracy=0.9556, micro_step_time=0.0612, step_time=0.1251, lr=2.0605e-04, scale=1.0000
[2025-12-10 02:40:51] [INFO]  train | epoch 012:     231 /  1429  global_step=15950, loss=0.1303, nll_loss=0.1303, accuracy=0.9617, micro_step_time=0.0612, step_time=0.1251, lr=2.0469e-04, scale=1.0000
[2025-12-10 02:40:57] [INFO]  train | epoch 012:     281 /  1429  global_step=16000, loss=0.1424, nll_loss=0.1424, accuracy=0.9573, micro_step_time=0.0613, step_time=0.1253, lr=2.0334e-04, scale=1.0000
[2025-12-10 02:41:04] [INFO]  train | epoch 012:     331 /  1429  global_step=16050, loss=0.1448, nll_loss=0.1448, accuracy=0.9569, micro_step_time=0.0612, step_time=0.1251, lr=2.0199e-04, scale=1.0000
[2025-12-10 02:41:10] [INFO]  train | epoch 012:     381 /  1429  global_step=16100, loss=0.1395, nll_loss=0.1395, accuracy=0.9581, micro_step_time=0.0612, step_time=0.1251, lr=2.0065e-04, scale=1.0000
[2025-12-10 02:41:16] [INFO]  train | epoch 012:     431 /  1429  global_step=16150, loss=0.1439, nll_loss=0.1439, accuracy=0.9563, micro_step_time=0.0612, step_time=0.1251, lr=1.9930e-04, scale=1.0000
[2025-12-10 02:41:22] [INFO]  train | epoch 012:     481 /  1429  global_step=16200, loss=0.1544, nll_loss=0.1544, accuracy=0.9540, micro_step_time=0.0612, step_time=0.1251, lr=1.9796e-04, scale=1.0000
[2025-12-10 02:41:29] [INFO]  train | epoch 012:     531 /  1429  global_step=16250, loss=0.1359, nll_loss=0.1359, accuracy=0.9585, micro_step_time=0.0612, step_time=0.1250, lr=1.9661e-04, scale=1.0000
[2025-12-10 02:41:35] [INFO]  train | epoch 012:     581 /  1429  global_step=16300, loss=0.1534, nll_loss=0.1534, accuracy=0.9541, micro_step_time=0.0612, step_time=0.1251, lr=1.9527e-04, scale=1.0000
[2025-12-10 02:41:41] [INFO]  train | epoch 012:     631 /  1429  global_step=16350, loss=0.1464, nll_loss=0.1464, accuracy=0.9566, micro_step_time=0.0612, step_time=0.1251, lr=1.9393e-04, scale=1.0000
[2025-12-10 02:41:47] [INFO]  train | epoch 012:     681 /  1429  global_step=16400, loss=0.1542, nll_loss=0.1542, accuracy=0.9539, micro_step_time=0.0612, step_time=0.1250, lr=1.9260e-04, scale=1.0000
[2025-12-10 02:41:54] [INFO]  train | epoch 012:     731 /  1429  global_step=16450, loss=0.1446, nll_loss=0.1446, accuracy=0.9561, micro_step_time=0.0612, step_time=0.1251, lr=1.9126e-04, scale=1.0000
[2025-12-10 02:42:00] [INFO]  train | epoch 012:     781 /  1429  global_step=16500, loss=0.1365, nll_loss=0.1365, accuracy=0.9593, micro_step_time=0.0612, step_time=0.1251, lr=1.8993e-04, scale=1.0000
[2025-12-10 02:42:06] [INFO]  train | epoch 012:     831 /  1429  global_step=16550, loss=0.1562, nll_loss=0.1562, accuracy=0.9557, micro_step_time=0.0612, step_time=0.1251, lr=1.8859e-04, scale=1.0000
[2025-12-10 02:42:12] [INFO]  train | epoch 012:     881 /  1429  global_step=16600, loss=0.1488, nll_loss=0.1488, accuracy=0.9550, micro_step_time=0.0612, step_time=0.1250, lr=1.8726e-04, scale=1.0000
[2025-12-10 02:42:19] [INFO]  train | epoch 012:     931 /  1429  global_step=16650, loss=0.1453, nll_loss=0.1453, accuracy=0.9558, micro_step_time=0.0613, step_time=0.1252, lr=1.8593e-04, scale=1.0000
[2025-12-10 02:42:25] [INFO]  train | epoch 012:     981 /  1429  global_step=16700, loss=0.1536, nll_loss=0.1536, accuracy=0.9536, micro_step_time=0.0612, step_time=0.1251, lr=1.8461e-04, scale=1.0000
[2025-12-10 02:42:31] [INFO]  train | epoch 012:    1031 /  1429  global_step=16750, loss=0.1507, nll_loss=0.1507, accuracy=0.9552, micro_step_time=0.0612, step_time=0.1251, lr=1.8328e-04, scale=1.0000
[2025-12-10 02:42:37] [INFO]  train | epoch 012:    1081 /  1429  global_step=16800, loss=0.1626, nll_loss=0.1626, accuracy=0.9513, micro_step_time=0.0613, step_time=0.1252, lr=1.8196e-04, scale=1.0000
[2025-12-10 02:42:44] [INFO]  train | epoch 012:    1131 /  1429  global_step=16850, loss=0.1544, nll_loss=0.1544, accuracy=0.9528, micro_step_time=0.0613, step_time=0.1253, lr=1.8064e-04, scale=1.0000
[2025-12-10 02:42:50] [INFO]  train | epoch 012:    1181 /  1429  global_step=16900, loss=0.1444, nll_loss=0.1444, accuracy=0.9574, micro_step_time=0.0612, step_time=0.1251, lr=1.7932e-04, scale=1.0000
[2025-12-10 02:42:56] [INFO]  train | epoch 012:    1231 /  1429  global_step=16950, loss=0.1531, nll_loss=0.1531, accuracy=0.9535, micro_step_time=0.0612, step_time=0.1251, lr=1.7800e-04, scale=1.0000
[2025-12-10 02:43:02] [INFO]  train | epoch 012:    1281 /  1429  global_step=17000, loss=0.1616, nll_loss=0.1616, accuracy=0.9504, micro_step_time=0.0612, step_time=0.1251, lr=1.7669e-04, scale=1.0000
[2025-12-10 02:43:09] [INFO]  train | epoch 012:    1331 /  1429  global_step=17050, loss=0.1500, nll_loss=0.1500, accuracy=0.9540, micro_step_time=0.0613, step_time=0.1252, lr=1.7538e-04, scale=1.0000
[2025-12-10 02:43:15] [INFO]  train | epoch 012:    1381 /  1429  global_step=17100, loss=0.1494, nll_loss=0.1494, accuracy=0.9538, micro_step_time=0.0612, step_time=0.1251, lr=1.7407e-04, scale=1.0000
[2025-12-10 02:43:21] [INFO]  End of epoch 12
[2025-12-10 02:43:21] [INFO]  train | epoch 012 | loss 0.1457 | nll_loss 0.1457 | kd_loss 0.0000
[2025-12-10 02:43:21] [INFO]  Evaluating before saving model...
[2025-12-10 02:43:21] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:44:23] [INFO]  eval_results in run@1: {'exact_match': 3.0, 'rougeL': 23.493}
[2025-12-10 02:45:21] [INFO]  eval_results in run@2: {'exact_match': 3.2, 'rougeL': 23.7517}
[2025-12-10 02:46:19] [INFO]  eval_results in run@3: {'exact_match': 3.1, 'rougeL': 23.1212}
[2025-12-10 02:46:19] [INFO]  dev | {'loss': 5.903292, 'token_num': 75795, 'token_acc': 0.391846, 'top1_prob': 0.772821} | {'exact_match': 3.1, 'rougeL': 23.4553}
[2025-12-10 02:46:19] [INFO]  Saving tokenizer...
[2025-12-10 02:46:19] [INFO]  Saving model...
[2025-12-10 02:46:19] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch12_step17148_loss5.9033_rougel23.4553
[2025-12-10 02:46:19] [INFO]  Start iterations of epoch 13
[2025-12-10 02:46:19] [INFO]  train | epoch 013:       2 /  1429  global_step=17150, loss=0.1516, nll_loss=0.1516, accuracy=0.9552, micro_step_time=0.0611, step_time=0.1248, lr=1.7276e-04, scale=1.0000
[2025-12-10 02:46:26] [INFO]  train | epoch 013:      52 /  1429  global_step=17200, loss=0.1026, nll_loss=0.1026, accuracy=0.9713, micro_step_time=0.0606, step_time=0.1240, lr=1.7145e-04, scale=1.0000
[2025-12-10 02:46:32] [INFO]  train | epoch 013:     102 /  1429  global_step=17250, loss=0.0925, nll_loss=0.0925, accuracy=0.9741, micro_step_time=0.0608, step_time=0.1242, lr=1.7015e-04, scale=1.0000
[2025-12-10 02:46:38] [INFO]  train | epoch 013:     152 /  1429  global_step=17300, loss=0.1005, nll_loss=0.1005, accuracy=0.9718, micro_step_time=0.0610, step_time=0.1247, lr=1.6885e-04, scale=1.0000
[2025-12-10 02:46:43] [INFO]  train | epoch 013:     202 /  1429  global_step=17350, loss=0.1015, nll_loss=0.1015, accuracy=0.9707, micro_step_time=0.0516, step_time=0.1054, lr=1.6755e-04, scale=1.0000
[2025-12-10 02:46:49] [INFO]  train | epoch 013:     252 /  1429  global_step=17400, loss=0.1077, nll_loss=0.1077, accuracy=0.9673, micro_step_time=0.0554, step_time=0.1130, lr=1.6626e-04, scale=1.0000
[2025-12-10 02:46:55] [INFO]  train | epoch 013:     302 /  1429  global_step=17450, loss=0.1030, nll_loss=0.1030, accuracy=0.9699, micro_step_time=0.0608, step_time=0.1244, lr=1.6496e-04, scale=1.0000
[2025-12-10 02:47:01] [INFO]  train | epoch 013:     352 /  1429  global_step=17500, loss=0.0988, nll_loss=0.0988, accuracy=0.9700, micro_step_time=0.0607, step_time=0.1242, lr=1.6367e-04, scale=1.0000
[2025-12-10 02:47:08] [INFO]  train | epoch 013:     402 /  1429  global_step=17550, loss=0.0982, nll_loss=0.0982, accuracy=0.9711, micro_step_time=0.0609, step_time=0.1245, lr=1.6239e-04, scale=1.0000
[2025-12-10 02:47:14] [INFO]  train | epoch 013:     452 /  1429  global_step=17600, loss=0.1060, nll_loss=0.1060, accuracy=0.9695, micro_step_time=0.0613, step_time=0.1252, lr=1.6110e-04, scale=1.0000
[2025-12-10 02:47:20] [INFO]  train | epoch 013:     502 /  1429  global_step=17650, loss=0.1087, nll_loss=0.1087, accuracy=0.9678, micro_step_time=0.0634, step_time=0.1297, lr=1.5982e-04, scale=1.0000
[2025-12-10 02:47:27] [INFO]  train | epoch 013:     552 /  1429  global_step=17700, loss=0.0890, nll_loss=0.0890, accuracy=0.9739, micro_step_time=0.0619, step_time=0.1268, lr=1.5854e-04, scale=1.0000
[2025-12-10 02:47:33] [INFO]  train | epoch 013:     602 /  1429  global_step=17750, loss=0.1100, nll_loss=0.1100, accuracy=0.9673, micro_step_time=0.0608, step_time=0.1245, lr=1.5726e-04, scale=1.0000
[2025-12-10 02:47:39] [INFO]  train | epoch 013:     652 /  1429  global_step=17800, loss=0.1103, nll_loss=0.1103, accuracy=0.9674, micro_step_time=0.0608, step_time=0.1244, lr=1.5599e-04, scale=1.0000
[2025-12-10 02:47:45] [INFO]  train | epoch 013:     702 /  1429  global_step=17850, loss=0.1021, nll_loss=0.1021, accuracy=0.9684, micro_step_time=0.0606, step_time=0.1239, lr=1.5472e-04, scale=1.0000
[2025-12-10 02:47:52] [INFO]  train | epoch 013:     752 /  1429  global_step=17900, loss=0.1069, nll_loss=0.1069, accuracy=0.9686, micro_step_time=0.0614, step_time=0.1257, lr=1.5345e-04, scale=1.0000
[2025-12-10 02:47:58] [INFO]  train | epoch 013:     802 /  1429  global_step=17950, loss=0.1040, nll_loss=0.1040, accuracy=0.9693, micro_step_time=0.0622, step_time=0.1274, lr=1.5218e-04, scale=1.0000
[2025-12-10 02:48:04] [INFO]  train | epoch 013:     852 /  1429  global_step=18000, loss=0.1083, nll_loss=0.1083, accuracy=0.9686, micro_step_time=0.0606, step_time=0.1241, lr=1.5092e-04, scale=1.0000
[2025-12-10 02:48:10] [INFO]  train | epoch 013:     902 /  1429  global_step=18050, loss=0.0970, nll_loss=0.0970, accuracy=0.9713, micro_step_time=0.0607, step_time=0.1242, lr=1.4966e-04, scale=1.0000
[2025-12-10 02:48:17] [INFO]  train | epoch 013:     952 /  1429  global_step=18100, loss=0.1143, nll_loss=0.1143, accuracy=0.9663, micro_step_time=0.0608, step_time=0.1244, lr=1.4840e-04, scale=1.0000
[2025-12-10 02:48:23] [INFO]  train | epoch 013:    1002 /  1429  global_step=18150, loss=0.1101, nll_loss=0.1101, accuracy=0.9672, micro_step_time=0.0608, step_time=0.1243, lr=1.4715e-04, scale=1.0000
[2025-12-10 02:48:29] [INFO]  train | epoch 013:    1052 /  1429  global_step=18200, loss=0.1107, nll_loss=0.1107, accuracy=0.9673, micro_step_time=0.0611, step_time=0.1250, lr=1.4590e-04, scale=1.0000
[2025-12-10 02:48:35] [INFO]  train | epoch 013:    1102 /  1429  global_step=18250, loss=0.1046, nll_loss=0.1046, accuracy=0.9694, micro_step_time=0.0608, step_time=0.1243, lr=1.4465e-04, scale=1.0000
[2025-12-10 02:48:42] [INFO]  train | epoch 013:    1152 /  1429  global_step=18300, loss=0.1002, nll_loss=0.1002, accuracy=0.9685, micro_step_time=0.0604, step_time=0.1235, lr=1.4341e-04, scale=1.0000
[2025-12-10 02:48:48] [INFO]  train | epoch 013:    1202 /  1429  global_step=18350, loss=0.1004, nll_loss=0.1004, accuracy=0.9710, micro_step_time=0.0604, step_time=0.1235, lr=1.4217e-04, scale=1.0000
[2025-12-10 02:48:54] [INFO]  train | epoch 013:    1252 /  1429  global_step=18400, loss=0.0981, nll_loss=0.0981, accuracy=0.9721, micro_step_time=0.0605, step_time=0.1237, lr=1.4093e-04, scale=1.0000
[2025-12-10 02:49:00] [INFO]  train | epoch 013:    1302 /  1429  global_step=18450, loss=0.1079, nll_loss=0.1079, accuracy=0.9674, micro_step_time=0.0607, step_time=0.1242, lr=1.3970e-04, scale=1.0000
[2025-12-10 02:49:06] [INFO]  train | epoch 013:    1352 /  1429  global_step=18500, loss=0.1022, nll_loss=0.1022, accuracy=0.9685, micro_step_time=0.0608, step_time=0.1243, lr=1.3846e-04, scale=1.0000
[2025-12-10 02:49:13] [INFO]  train | epoch 013:    1402 /  1429  global_step=18550, loss=0.1084, nll_loss=0.1084, accuracy=0.9683, micro_step_time=0.0604, step_time=0.1236, lr=1.3724e-04, scale=1.0000
[2025-12-10 02:49:16] [INFO]  End of epoch 13
[2025-12-10 02:49:16] [INFO]  train | epoch 013 | loss 0.1069 | nll_loss 0.1069 | kd_loss 0.0000
[2025-12-10 02:49:16] [INFO]  Evaluating before saving model...
[2025-12-10 02:49:16] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:50:19] [INFO]  eval_results in run@1: {'exact_match': 3.4, 'rougeL': 23.9879}
[2025-12-10 02:51:14] [INFO]  eval_results in run@2: {'exact_match': 3.2, 'rougeL': 23.8202}
[2025-12-10 02:52:15] [INFO]  eval_results in run@3: {'exact_match': 3.3, 'rougeL': 23.7208}
[2025-12-10 02:52:15] [INFO]  dev | {'loss': 6.038815, 'token_num': 75795, 'token_acc': 0.392031, 'top1_prob': 0.782901} | {'exact_match': 3.3, 'rougeL': 23.843}
[2025-12-10 02:52:15] [INFO]  Saving tokenizer...
[2025-12-10 02:52:15] [INFO]  Saving model...
[2025-12-10 02:52:15] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch13_step18577_loss6.0388_rougel23.8430
[2025-12-10 02:52:15] [INFO]  Start iterations of epoch 14
[2025-12-10 02:52:18] [INFO]  train | epoch 014:      23 /  1429  global_step=18600, loss=0.0841, nll_loss=0.0841, accuracy=0.9760, micro_step_time=0.0603, step_time=0.1232, lr=1.3601e-04, scale=1.0000
[2025-12-10 02:52:24] [INFO]  train | epoch 014:      73 /  1429  global_step=18650, loss=0.0668, nll_loss=0.0668, accuracy=0.9817, micro_step_time=0.0609, step_time=0.1246, lr=1.3479e-04, scale=1.0000
[2025-12-10 02:52:31] [INFO]  train | epoch 014:     123 /  1429  global_step=18700, loss=0.0616, nll_loss=0.0616, accuracy=0.9829, micro_step_time=0.0609, step_time=0.1245, lr=1.3357e-04, scale=1.0000
[2025-12-10 02:52:37] [INFO]  train | epoch 014:     173 /  1429  global_step=18750, loss=0.0702, nll_loss=0.0702, accuracy=0.9797, micro_step_time=0.0609, step_time=0.1245, lr=1.3236e-04, scale=1.0000
[2025-12-10 02:52:43] [INFO]  train | epoch 014:     223 /  1429  global_step=18800, loss=0.0654, nll_loss=0.0654, accuracy=0.9810, micro_step_time=0.0609, step_time=0.1246, lr=1.3115e-04, scale=1.0000
[2025-12-10 02:52:49] [INFO]  train | epoch 014:     273 /  1429  global_step=18850, loss=0.0664, nll_loss=0.0664, accuracy=0.9819, micro_step_time=0.0609, step_time=0.1245, lr=1.2994e-04, scale=1.0000
[2025-12-10 02:52:56] [INFO]  train | epoch 014:     323 /  1429  global_step=18900, loss=0.0708, nll_loss=0.0708, accuracy=0.9806, micro_step_time=0.0609, step_time=0.1246, lr=1.2874e-04, scale=1.0000
[2025-12-10 02:53:02] [INFO]  train | epoch 014:     373 /  1429  global_step=18950, loss=0.0667, nll_loss=0.0667, accuracy=0.9812, micro_step_time=0.0609, step_time=0.1246, lr=1.2754e-04, scale=1.0000
[2025-12-10 02:53:08] [INFO]  train | epoch 014:     423 /  1429  global_step=19000, loss=0.0726, nll_loss=0.0726, accuracy=0.9785, micro_step_time=0.0609, step_time=0.1246, lr=1.2635e-04, scale=1.0000
[2025-12-10 02:53:14] [INFO]  train | epoch 014:     473 /  1429  global_step=19050, loss=0.0683, nll_loss=0.0683, accuracy=0.9799, micro_step_time=0.0610, step_time=0.1246, lr=1.2515e-04, scale=1.0000
[2025-12-10 02:53:20] [INFO]  train | epoch 014:     523 /  1429  global_step=19100, loss=0.0687, nll_loss=0.0687, accuracy=0.9804, micro_step_time=0.0609, step_time=0.1246, lr=1.2397e-04, scale=1.0000
[2025-12-10 02:53:27] [INFO]  train | epoch 014:     573 /  1429  global_step=19150, loss=0.0748, nll_loss=0.0748, accuracy=0.9792, micro_step_time=0.0610, step_time=0.1247, lr=1.2278e-04, scale=1.0000
[2025-12-10 02:53:33] [INFO]  train | epoch 014:     623 /  1429  global_step=19200, loss=0.0672, nll_loss=0.0672, accuracy=0.9814, micro_step_time=0.0609, step_time=0.1245, lr=1.2160e-04, scale=1.0000
[2025-12-10 02:53:39] [INFO]  train | epoch 014:     673 /  1429  global_step=19250, loss=0.0691, nll_loss=0.0691, accuracy=0.9798, micro_step_time=0.0609, step_time=0.1245, lr=1.2043e-04, scale=1.0000
[2025-12-10 02:53:45] [INFO]  train | epoch 014:     723 /  1429  global_step=19300, loss=0.0781, nll_loss=0.0781, accuracy=0.9772, micro_step_time=0.0609, step_time=0.1245, lr=1.1925e-04, scale=1.0000
[2025-12-10 02:53:52] [INFO]  train | epoch 014:     773 /  1429  global_step=19350, loss=0.0768, nll_loss=0.0768, accuracy=0.9780, micro_step_time=0.0609, step_time=0.1245, lr=1.1808e-04, scale=1.0000
[2025-12-10 02:53:58] [INFO]  train | epoch 014:     823 /  1429  global_step=19400, loss=0.0747, nll_loss=0.0747, accuracy=0.9794, micro_step_time=0.0609, step_time=0.1245, lr=1.1692e-04, scale=1.0000
[2025-12-10 02:54:04] [INFO]  train | epoch 014:     873 /  1429  global_step=19450, loss=0.0707, nll_loss=0.0707, accuracy=0.9791, micro_step_time=0.0609, step_time=0.1244, lr=1.1576e-04, scale=1.0000
[2025-12-10 02:54:10] [INFO]  train | epoch 014:     923 /  1429  global_step=19500, loss=0.0744, nll_loss=0.0744, accuracy=0.9792, micro_step_time=0.0609, step_time=0.1245, lr=1.1460e-04, scale=1.0000
[2025-12-10 02:54:17] [INFO]  train | epoch 014:     973 /  1429  global_step=19550, loss=0.0795, nll_loss=0.0795, accuracy=0.9773, micro_step_time=0.0609, step_time=0.1245, lr=1.1345e-04, scale=1.0000
[2025-12-10 02:54:23] [INFO]  train | epoch 014:    1023 /  1429  global_step=19600, loss=0.0711, nll_loss=0.0711, accuracy=0.9799, micro_step_time=0.0609, step_time=0.1244, lr=1.1230e-04, scale=1.0000
[2025-12-10 02:54:29] [INFO]  train | epoch 014:    1073 /  1429  global_step=19650, loss=0.0754, nll_loss=0.0754, accuracy=0.9775, micro_step_time=0.0608, step_time=0.1244, lr=1.1116e-04, scale=1.0000
[2025-12-10 02:54:35] [INFO]  train | epoch 014:    1123 /  1429  global_step=19700, loss=0.0689, nll_loss=0.0689, accuracy=0.9797, micro_step_time=0.0609, step_time=0.1245, lr=1.1002e-04, scale=1.0000
[2025-12-10 02:54:41] [INFO]  train | epoch 014:    1173 /  1429  global_step=19750, loss=0.0658, nll_loss=0.0658, accuracy=0.9808, micro_step_time=0.0609, step_time=0.1246, lr=1.0888e-04, scale=1.0000
[2025-12-10 02:54:48] [INFO]  train | epoch 014:    1223 /  1429  global_step=19800, loss=0.0713, nll_loss=0.0713, accuracy=0.9790, micro_step_time=0.0609, step_time=0.1245, lr=1.0775e-04, scale=1.0000
[2025-12-10 02:54:54] [INFO]  train | epoch 014:    1273 /  1429  global_step=19850, loss=0.0732, nll_loss=0.0732, accuracy=0.9776, micro_step_time=0.0610, step_time=0.1246, lr=1.0662e-04, scale=1.0000
[2025-12-10 02:55:00] [INFO]  train | epoch 014:    1323 /  1429  global_step=19900, loss=0.0714, nll_loss=0.0714, accuracy=0.9792, micro_step_time=0.0609, step_time=0.1245, lr=1.0550e-04, scale=1.0000
[2025-12-10 02:55:06] [INFO]  train | epoch 014:    1373 /  1429  global_step=19950, loss=0.0657, nll_loss=0.0657, accuracy=0.9815, micro_step_time=0.0609, step_time=0.1245, lr=1.0438e-04, scale=1.0000
[2025-12-10 02:55:13] [INFO]  train | epoch 014:    1423 /  1429  global_step=20000, loss=0.0723, nll_loss=0.0723, accuracy=0.9801, micro_step_time=0.0609, step_time=0.1245, lr=1.0327e-04, scale=1.0000
[2025-12-10 02:55:13] [INFO]  End of epoch 14
[2025-12-10 02:55:13] [INFO]  train | epoch 014 | loss 0.0722 | nll_loss 0.0722 | kd_loss 0.0000
[2025-12-10 02:55:13] [INFO]  Evaluating before saving model...
[2025-12-10 02:55:13] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 02:56:22] [INFO]  eval_results in run@1: {'exact_match': 2.9, 'rougeL': 23.649}
[2025-12-10 02:57:30] [INFO]  eval_results in run@2: {'exact_match': 2.9, 'rougeL': 23.5626}
[2025-12-10 02:58:37] [INFO]  eval_results in run@3: {'exact_match': 3.0, 'rougeL': 23.3653}
[2025-12-10 02:58:37] [INFO]  dev | {'loss': 6.112277, 'token_num': 75795, 'token_acc': 0.394947, 'top1_prob': 0.789023} | {'exact_match': 2.9333, 'rougeL': 23.5256}
[2025-12-10 02:58:37] [INFO]  Saving tokenizer...
[2025-12-10 02:58:37] [INFO]  Saving model...
[2025-12-10 02:58:38] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch14_step20006_loss6.1123_rougel23.5256
[2025-12-10 02:58:38] [INFO]  Start iterations of epoch 15
[2025-12-10 02:58:43] [INFO]  train | epoch 015:      44 /  1429  global_step=20050, loss=0.0501, nll_loss=0.0501, accuracy=0.9869, micro_step_time=0.0607, step_time=0.1241, lr=1.0216e-04, scale=1.0000
[2025-12-10 02:58:49] [INFO]  train | epoch 015:      94 /  1429  global_step=20100, loss=0.0485, nll_loss=0.0485, accuracy=0.9874, micro_step_time=0.0609, step_time=0.1245, lr=1.0105e-04, scale=1.0000
[2025-12-10 02:58:56] [INFO]  train | epoch 015:     144 /  1429  global_step=20150, loss=0.0453, nll_loss=0.0453, accuracy=0.9885, micro_step_time=0.0609, step_time=0.1245, lr=9.9951e-05, scale=1.0000
[2025-12-10 02:59:02] [INFO]  train | epoch 015:     194 /  1429  global_step=20200, loss=0.0504, nll_loss=0.0504, accuracy=0.9856, micro_step_time=0.0610, step_time=0.1247, lr=9.8855e-05, scale=1.0000
[2025-12-10 02:59:08] [INFO]  train | epoch 015:     244 /  1429  global_step=20250, loss=0.0509, nll_loss=0.0509, accuracy=0.9865, micro_step_time=0.0608, step_time=0.1244, lr=9.7763e-05, scale=1.0000
[2025-12-10 02:59:14] [INFO]  train | epoch 015:     294 /  1429  global_step=20300, loss=0.0529, nll_loss=0.0529, accuracy=0.9855, micro_step_time=0.0609, step_time=0.1246, lr=9.6676e-05, scale=1.0000
[2025-12-10 02:59:20] [INFO]  train | epoch 015:     344 /  1429  global_step=20350, loss=0.0474, nll_loss=0.0474, accuracy=0.9863, micro_step_time=0.0609, step_time=0.1244, lr=9.5594e-05, scale=1.0000
[2025-12-10 02:59:27] [INFO]  train | epoch 015:     394 /  1429  global_step=20400, loss=0.0546, nll_loss=0.0546, accuracy=0.9847, micro_step_time=0.0609, step_time=0.1246, lr=9.4516e-05, scale=1.0000
[2025-12-10 02:59:33] [INFO]  train | epoch 015:     444 /  1429  global_step=20450, loss=0.0490, nll_loss=0.0490, accuracy=0.9873, micro_step_time=0.0609, step_time=0.1245, lr=9.3443e-05, scale=1.0000
[2025-12-10 02:59:39] [INFO]  train | epoch 015:     494 /  1429  global_step=20500, loss=0.0443, nll_loss=0.0443, accuracy=0.9881, micro_step_time=0.0609, step_time=0.1245, lr=9.2374e-05, scale=1.0000
[2025-12-10 02:59:45] [INFO]  train | epoch 015:     544 /  1429  global_step=20550, loss=0.0493, nll_loss=0.0493, accuracy=0.9867, micro_step_time=0.0609, step_time=0.1244, lr=9.1311e-05, scale=1.0000
[2025-12-10 02:59:52] [INFO]  train | epoch 015:     594 /  1429  global_step=20600, loss=0.0472, nll_loss=0.0472, accuracy=0.9879, micro_step_time=0.0609, step_time=0.1245, lr=9.0252e-05, scale=1.0000
[2025-12-10 02:59:58] [INFO]  train | epoch 015:     644 /  1429  global_step=20650, loss=0.0477, nll_loss=0.0477, accuracy=0.9870, micro_step_time=0.0610, step_time=0.1247, lr=8.9198e-05, scale=1.0000
[2025-12-10 03:00:04] [INFO]  train | epoch 015:     694 /  1429  global_step=20700, loss=0.0462, nll_loss=0.0462, accuracy=0.9864, micro_step_time=0.0609, step_time=0.1244, lr=8.8149e-05, scale=1.0000
[2025-12-10 03:00:10] [INFO]  train | epoch 015:     744 /  1429  global_step=20750, loss=0.0495, nll_loss=0.0495, accuracy=0.9861, micro_step_time=0.0609, step_time=0.1246, lr=8.7105e-05, scale=1.0000
[2025-12-10 03:00:17] [INFO]  train | epoch 015:     794 /  1429  global_step=20800, loss=0.0475, nll_loss=0.0475, accuracy=0.9878, micro_step_time=0.0609, step_time=0.1245, lr=8.6066e-05, scale=1.0000
[2025-12-10 03:00:23] [INFO]  train | epoch 015:     844 /  1429  global_step=20850, loss=0.0452, nll_loss=0.0452, accuracy=0.9871, micro_step_time=0.0609, step_time=0.1245, lr=8.5031e-05, scale=1.0000
[2025-12-10 03:00:29] [INFO]  train | epoch 015:     894 /  1429  global_step=20900, loss=0.0440, nll_loss=0.0440, accuracy=0.9879, micro_step_time=0.0608, step_time=0.1243, lr=8.4002e-05, scale=1.0000
[2025-12-10 03:00:35] [INFO]  train | epoch 015:     944 /  1429  global_step=20950, loss=0.0500, nll_loss=0.0500, accuracy=0.9862, micro_step_time=0.0609, step_time=0.1245, lr=8.2978e-05, scale=1.0000
[2025-12-10 03:00:41] [INFO]  train | epoch 015:     994 /  1429  global_step=21000, loss=0.0479, nll_loss=0.0479, accuracy=0.9870, micro_step_time=0.0609, step_time=0.1246, lr=8.1959e-05, scale=1.0000
[2025-12-10 03:00:48] [INFO]  train | epoch 015:    1044 /  1429  global_step=21050, loss=0.0471, nll_loss=0.0471, accuracy=0.9867, micro_step_time=0.0607, step_time=0.1240, lr=8.0944e-05, scale=1.0000
[2025-12-10 03:00:54] [INFO]  train | epoch 015:    1094 /  1429  global_step=21100, loss=0.0503, nll_loss=0.0503, accuracy=0.9862, micro_step_time=0.0608, step_time=0.1243, lr=7.9935e-05, scale=1.0000
[2025-12-10 03:01:00] [INFO]  train | epoch 015:    1144 /  1429  global_step=21150, loss=0.0503, nll_loss=0.0503, accuracy=0.9863, micro_step_time=0.0609, step_time=0.1245, lr=7.8931e-05, scale=1.0000
[2025-12-10 03:01:06] [INFO]  train | epoch 015:    1194 /  1429  global_step=21200, loss=0.0491, nll_loss=0.0491, accuracy=0.9877, micro_step_time=0.0608, step_time=0.1244, lr=7.7933e-05, scale=1.0000
[2025-12-10 03:01:13] [INFO]  train | epoch 015:    1244 /  1429  global_step=21250, loss=0.0508, nll_loss=0.0508, accuracy=0.9856, micro_step_time=0.0609, step_time=0.1244, lr=7.6939e-05, scale=1.0000
[2025-12-10 03:01:19] [INFO]  train | epoch 015:    1294 /  1429  global_step=21300, loss=0.0463, nll_loss=0.0463, accuracy=0.9879, micro_step_time=0.0608, step_time=0.1243, lr=7.5951e-05, scale=1.0000
[2025-12-10 03:01:25] [INFO]  train | epoch 015:    1344 /  1429  global_step=21350, loss=0.0474, nll_loss=0.0474, accuracy=0.9871, micro_step_time=0.0608, step_time=0.1243, lr=7.4968e-05, scale=1.0000
[2025-12-10 03:01:31] [INFO]  train | epoch 015:    1394 /  1429  global_step=21400, loss=0.0473, nll_loss=0.0473, accuracy=0.9870, micro_step_time=0.0613, step_time=0.1254, lr=7.3990e-05, scale=1.0000
[2025-12-10 03:01:36] [INFO]  End of epoch 15
[2025-12-10 03:01:36] [INFO]  train | epoch 015 | loss 0.0475 | nll_loss 0.0475 | kd_loss 0.0000
[2025-12-10 03:01:36] [INFO]  Evaluating before saving model...
[2025-12-10 03:01:36] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 03:02:42] [INFO]  eval_results in run@1: {'exact_match': 3.2, 'rougeL': 24.2969}
[2025-12-10 03:03:46] [INFO]  eval_results in run@2: {'exact_match': 3.1, 'rougeL': 24.0539}
[2025-12-10 03:04:45] [INFO]  eval_results in run@3: {'exact_match': 3.2, 'rougeL': 23.9454}
[2025-12-10 03:04:45] [INFO]  dev | {'loss': 6.178561, 'token_num': 75795, 'token_acc': 0.395646, 'top1_prob': 0.794037} | {'exact_match': 3.1667, 'rougeL': 24.0987}
[2025-12-10 03:04:45] [INFO]  Saving tokenizer...
[2025-12-10 03:04:45] [INFO]  Saving model...
[2025-12-10 03:04:46] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch15_step21435_loss6.1786_rougel24.0987
[2025-12-10 03:04:46] [INFO]  Start iterations of epoch 16
[2025-12-10 03:04:48] [INFO]  train | epoch 016:      15 /  1429  global_step=21450, loss=0.0446, nll_loss=0.0446, accuracy=0.9878, micro_step_time=0.0606, step_time=0.1239, lr=7.3017e-05, scale=1.0000
[2025-12-10 03:04:54] [INFO]  train | epoch 016:      65 /  1429  global_step=21500, loss=0.0349, nll_loss=0.0349, accuracy=0.9911, micro_step_time=0.0609, step_time=0.1246, lr=7.2050e-05, scale=1.0000
[2025-12-10 03:05:00] [INFO]  train | epoch 016:     115 /  1429  global_step=21550, loss=0.0324, nll_loss=0.0324, accuracy=0.9916, micro_step_time=0.0609, step_time=0.1246, lr=7.1089e-05, scale=1.0000
[2025-12-10 03:05:06] [INFO]  train | epoch 016:     165 /  1429  global_step=21600, loss=0.0330, nll_loss=0.0330, accuracy=0.9915, micro_step_time=0.0609, step_time=0.1244, lr=7.0132e-05, scale=1.0000
[2025-12-10 03:05:13] [INFO]  train | epoch 016:     215 /  1429  global_step=21650, loss=0.0345, nll_loss=0.0345, accuracy=0.9906, micro_step_time=0.0609, step_time=0.1244, lr=6.9181e-05, scale=1.0000
[2025-12-10 03:05:18] [INFO]  train | epoch 016:     265 /  1429  global_step=21700, loss=0.0363, nll_loss=0.0363, accuracy=0.9897, micro_step_time=0.0517, step_time=0.1056, lr=6.8236e-05, scale=1.0000
[2025-12-10 03:05:22] [INFO]  train | epoch 016:     315 /  1429  global_step=21750, loss=0.0349, nll_loss=0.0349, accuracy=0.9905, micro_step_time=0.0432, step_time=0.0883, lr=6.7296e-05, scale=1.0000
[2025-12-10 03:05:27] [INFO]  train | epoch 016:     365 /  1429  global_step=21800, loss=0.0330, nll_loss=0.0330, accuracy=0.9917, micro_step_time=0.0432, step_time=0.0883, lr=6.6362e-05, scale=1.0000
[2025-12-10 03:05:31] [INFO]  train | epoch 016:     415 /  1429  global_step=21850, loss=0.0315, nll_loss=0.0315, accuracy=0.9923, micro_step_time=0.0433, step_time=0.0884, lr=6.5433e-05, scale=1.0000
[2025-12-10 03:05:35] [INFO]  train | epoch 016:     465 /  1429  global_step=21900, loss=0.0372, nll_loss=0.0372, accuracy=0.9908, micro_step_time=0.0432, step_time=0.0883, lr=6.4509e-05, scale=1.0000
[2025-12-10 03:05:40] [INFO]  train | epoch 016:     515 /  1429  global_step=21950, loss=0.0336, nll_loss=0.0336, accuracy=0.9916, micro_step_time=0.0433, step_time=0.0884, lr=6.3592e-05, scale=1.0000
[2025-12-10 03:05:44] [INFO]  train | epoch 016:     565 /  1429  global_step=22000, loss=0.0385, nll_loss=0.0385, accuracy=0.9899, micro_step_time=0.0432, step_time=0.0882, lr=6.2680e-05, scale=1.0000
[2025-12-10 03:05:49] [INFO]  train | epoch 016:     615 /  1429  global_step=22050, loss=0.0387, nll_loss=0.0387, accuracy=0.9900, micro_step_time=0.0432, step_time=0.0882, lr=6.1773e-05, scale=1.0000
[2025-12-10 03:05:53] [INFO]  train | epoch 016:     665 /  1429  global_step=22100, loss=0.0324, nll_loss=0.0324, accuracy=0.9916, micro_step_time=0.0432, step_time=0.0882, lr=6.0873e-05, scale=1.0000
[2025-12-10 03:05:58] [INFO]  train | epoch 016:     715 /  1429  global_step=22150, loss=0.0354, nll_loss=0.0354, accuracy=0.9916, micro_step_time=0.0432, step_time=0.0882, lr=5.9978e-05, scale=1.0000
[2025-12-10 03:06:02] [INFO]  train | epoch 016:     765 /  1429  global_step=22200, loss=0.0312, nll_loss=0.0312, accuracy=0.9922, micro_step_time=0.0432, step_time=0.0883, lr=5.9088e-05, scale=1.0000
[2025-12-10 03:06:06] [INFO]  train | epoch 016:     815 /  1429  global_step=22250, loss=0.0350, nll_loss=0.0350, accuracy=0.9905, micro_step_time=0.0432, step_time=0.0882, lr=5.8205e-05, scale=1.0000
[2025-12-10 03:06:11] [INFO]  train | epoch 016:     865 /  1429  global_step=22300, loss=0.0304, nll_loss=0.0304, accuracy=0.9924, micro_step_time=0.0432, step_time=0.0883, lr=5.7327e-05, scale=1.0000
[2025-12-10 03:06:15] [INFO]  train | epoch 016:     915 /  1429  global_step=22350, loss=0.0345, nll_loss=0.0345, accuracy=0.9911, micro_step_time=0.0432, step_time=0.0882, lr=5.6455e-05, scale=1.0000
[2025-12-10 03:06:20] [INFO]  train | epoch 016:     965 /  1429  global_step=22400, loss=0.0331, nll_loss=0.0331, accuracy=0.9919, micro_step_time=0.0432, step_time=0.0882, lr=5.5589e-05, scale=1.0000
[2025-12-10 03:06:24] [INFO]  train | epoch 016:    1015 /  1429  global_step=22450, loss=0.0335, nll_loss=0.0335, accuracy=0.9920, micro_step_time=0.0432, step_time=0.0882, lr=5.4729e-05, scale=1.0000
[2025-12-10 03:06:28] [INFO]  train | epoch 016:    1065 /  1429  global_step=22500, loss=0.0338, nll_loss=0.0338, accuracy=0.9910, micro_step_time=0.0432, step_time=0.0883, lr=5.3875e-05, scale=1.0000
[2025-12-10 03:06:33] [INFO]  train | epoch 016:    1115 /  1429  global_step=22550, loss=0.0328, nll_loss=0.0328, accuracy=0.9916, micro_step_time=0.0432, step_time=0.0882, lr=5.3027e-05, scale=1.0000
[2025-12-10 03:06:37] [INFO]  train | epoch 016:    1165 /  1429  global_step=22600, loss=0.0317, nll_loss=0.0317, accuracy=0.9921, micro_step_time=0.0432, step_time=0.0883, lr=5.2184e-05, scale=1.0000
[2025-12-10 03:06:42] [INFO]  train | epoch 016:    1215 /  1429  global_step=22650, loss=0.0329, nll_loss=0.0329, accuracy=0.9912, micro_step_time=0.0432, step_time=0.0882, lr=5.1348e-05, scale=1.0000
[2025-12-10 03:06:46] [INFO]  train | epoch 016:    1265 /  1429  global_step=22700, loss=0.0315, nll_loss=0.0315, accuracy=0.9921, micro_step_time=0.0432, step_time=0.0883, lr=5.0517e-05, scale=1.0000
[2025-12-10 03:06:50] [INFO]  train | epoch 016:    1315 /  1429  global_step=22750, loss=0.0296, nll_loss=0.0296, accuracy=0.9923, micro_step_time=0.0432, step_time=0.0883, lr=4.9693e-05, scale=1.0000
[2025-12-10 03:06:55] [INFO]  train | epoch 016:    1365 /  1429  global_step=22800, loss=0.0326, nll_loss=0.0326, accuracy=0.9920, micro_step_time=0.0432, step_time=0.0883, lr=4.8875e-05, scale=1.0000
[2025-12-10 03:06:59] [INFO]  train | epoch 016:    1415 /  1429  global_step=22850, loss=0.0352, nll_loss=0.0352, accuracy=0.9913, micro_step_time=0.0432, step_time=0.0882, lr=4.8063e-05, scale=1.0000
[2025-12-10 03:07:01] [INFO]  End of epoch 16
[2025-12-10 03:07:01] [INFO]  train | epoch 016 | loss 0.0346 | nll_loss 0.0346 | kd_loss 0.0000
[2025-12-10 03:07:01] [INFO]  Evaluating before saving model...
[2025-12-10 03:07:01] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 03:08:05] [INFO]  eval_results in run@1: {'exact_match': 3.0, 'rougeL': 24.3617}
[2025-12-10 03:09:06] [INFO]  eval_results in run@2: {'exact_match': 3.2, 'rougeL': 24.0523}
[2025-12-10 03:10:06] [INFO]  eval_results in run@3: {'exact_match': 3.1, 'rougeL': 24.4333}
[2025-12-10 03:10:06] [INFO]  dev | {'loss': 6.252022, 'token_num': 75795, 'token_acc': 0.397915, 'top1_prob': 0.798232} | {'exact_match': 3.1, 'rougeL': 24.2824}
[2025-12-10 03:10:06] [INFO]  Saving tokenizer...
[2025-12-10 03:10:06] [INFO]  Saving model...
[2025-12-10 03:10:06] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch16_step22864_loss6.2520_rougel24.2824
[2025-12-10 03:10:06] [INFO]  Start iterations of epoch 17
[2025-12-10 03:10:09] [INFO]  train | epoch 017:      36 /  1429  global_step=22900, loss=0.0264, nll_loss=0.0264, accuracy=0.9934, micro_step_time=0.0435, step_time=0.0888, lr=4.7256e-05, scale=1.0000
[2025-12-10 03:10:14] [INFO]  train | epoch 017:      86 /  1429  global_step=22950, loss=0.0261, nll_loss=0.0261, accuracy=0.9934, micro_step_time=0.0433, step_time=0.0884, lr=4.6456e-05, scale=1.0000
[2025-12-10 03:10:18] [INFO]  train | epoch 017:     136 /  1429  global_step=23000, loss=0.0252, nll_loss=0.0252, accuracy=0.9942, micro_step_time=0.0432, step_time=0.0883, lr=4.5663e-05, scale=1.0000
[2025-12-10 03:10:24] [INFO]  train | epoch 017:     186 /  1429  global_step=23050, loss=0.0249, nll_loss=0.0249, accuracy=0.9944, micro_step_time=0.0535, step_time=0.1093, lr=4.4875e-05, scale=1.0000
[2025-12-10 03:10:30] [INFO]  train | epoch 017:     236 /  1429  global_step=23100, loss=0.0240, nll_loss=0.0240, accuracy=0.9949, micro_step_time=0.0583, step_time=0.1193, lr=4.4093e-05, scale=1.0000
[2025-12-10 03:10:36] [INFO]  train | epoch 017:     286 /  1429  global_step=23150, loss=0.0234, nll_loss=0.0234, accuracy=0.9945, micro_step_time=0.0582, step_time=0.1190, lr=4.3318e-05, scale=1.0000
[2025-12-10 03:10:42] [INFO]  train | epoch 017:     336 /  1429  global_step=23200, loss=0.0256, nll_loss=0.0256, accuracy=0.9944, micro_step_time=0.0582, step_time=0.1190, lr=4.2549e-05, scale=1.0000
[2025-12-10 03:10:48] [INFO]  train | epoch 017:     386 /  1429  global_step=23250, loss=0.0261, nll_loss=0.0261, accuracy=0.9939, micro_step_time=0.0583, step_time=0.1192, lr=4.1786e-05, scale=1.0000
[2025-12-10 03:10:53] [INFO]  train | epoch 017:     436 /  1429  global_step=23300, loss=0.0265, nll_loss=0.0265, accuracy=0.9935, micro_step_time=0.0568, step_time=0.1161, lr=4.1030e-05, scale=1.0000
[2025-12-10 03:10:59] [INFO]  train | epoch 017:     486 /  1429  global_step=23350, loss=0.0259, nll_loss=0.0259, accuracy=0.9940, micro_step_time=0.0582, step_time=0.1190, lr=4.0280e-05, scale=1.0000
[2025-12-10 03:11:05] [INFO]  train | epoch 017:     536 /  1429  global_step=23400, loss=0.0238, nll_loss=0.0238, accuracy=0.9942, micro_step_time=0.0582, step_time=0.1190, lr=3.9536e-05, scale=1.0000
[2025-12-10 03:11:11] [INFO]  train | epoch 017:     586 /  1429  global_step=23450, loss=0.0276, nll_loss=0.0276, accuracy=0.9928, micro_step_time=0.0582, step_time=0.1190, lr=3.8799e-05, scale=1.0000
[2025-12-10 03:11:17] [INFO]  train | epoch 017:     636 /  1429  global_step=23500, loss=0.0245, nll_loss=0.0245, accuracy=0.9943, micro_step_time=0.0582, step_time=0.1191, lr=3.8067e-05, scale=1.0000
[2025-12-10 03:11:23] [INFO]  train | epoch 017:     686 /  1429  global_step=23550, loss=0.0253, nll_loss=0.0253, accuracy=0.9936, micro_step_time=0.0582, step_time=0.1191, lr=3.7343e-05, scale=1.0000
[2025-12-10 03:11:29] [INFO]  train | epoch 017:     736 /  1429  global_step=23600, loss=0.0262, nll_loss=0.0262, accuracy=0.9935, micro_step_time=0.0582, step_time=0.1191, lr=3.6625e-05, scale=1.0000
[2025-12-10 03:11:35] [INFO]  train | epoch 017:     786 /  1429  global_step=23650, loss=0.0232, nll_loss=0.0232, accuracy=0.9940, micro_step_time=0.0581, step_time=0.1189, lr=3.5913e-05, scale=1.0000
[2025-12-10 03:11:41] [INFO]  train | epoch 017:     836 /  1429  global_step=23700, loss=0.0270, nll_loss=0.0270, accuracy=0.9935, micro_step_time=0.0582, step_time=0.1190, lr=3.5207e-05, scale=1.0000
[2025-12-10 03:11:47] [INFO]  train | epoch 017:     886 /  1429  global_step=23750, loss=0.0236, nll_loss=0.0236, accuracy=0.9945, micro_step_time=0.0581, step_time=0.1189, lr=3.4509e-05, scale=1.0000
[2025-12-10 03:11:53] [INFO]  train | epoch 017:     936 /  1429  global_step=23800, loss=0.0264, nll_loss=0.0264, accuracy=0.9940, micro_step_time=0.0582, step_time=0.1190, lr=3.3816e-05, scale=1.0000
[2025-12-10 03:11:59] [INFO]  train | epoch 017:     986 /  1429  global_step=23850, loss=0.0238, nll_loss=0.0238, accuracy=0.9944, micro_step_time=0.0581, step_time=0.1189, lr=3.3130e-05, scale=1.0000
[2025-12-10 03:12:05] [INFO]  train | epoch 017:    1036 /  1429  global_step=23900, loss=0.0246, nll_loss=0.0246, accuracy=0.9937, micro_step_time=0.0581, step_time=0.1188, lr=3.2451e-05, scale=1.0000
[2025-12-10 03:12:11] [INFO]  train | epoch 017:    1086 /  1429  global_step=23950, loss=0.0216, nll_loss=0.0216, accuracy=0.9950, micro_step_time=0.0582, step_time=0.1190, lr=3.1779e-05, scale=1.0000
[2025-12-10 03:12:17] [INFO]  train | epoch 017:    1136 /  1429  global_step=24000, loss=0.0254, nll_loss=0.0254, accuracy=0.9937, micro_step_time=0.0580, step_time=0.1185, lr=3.1113e-05, scale=1.0000
[2025-12-10 03:12:23] [INFO]  train | epoch 017:    1186 /  1429  global_step=24050, loss=0.0259, nll_loss=0.0259, accuracy=0.9935, micro_step_time=0.0583, step_time=0.1192, lr=3.0453e-05, scale=1.0000
[2025-12-10 03:12:29] [INFO]  train | epoch 017:    1236 /  1429  global_step=24100, loss=0.0251, nll_loss=0.0251, accuracy=0.9943, micro_step_time=0.0582, step_time=0.1190, lr=2.9800e-05, scale=1.0000
[2025-12-10 03:12:35] [INFO]  train | epoch 017:    1286 /  1429  global_step=24150, loss=0.0262, nll_loss=0.0262, accuracy=0.9934, micro_step_time=0.0581, step_time=0.1190, lr=2.9154e-05, scale=1.0000
[2025-12-10 03:12:40] [INFO]  train | epoch 017:    1336 /  1429  global_step=24200, loss=0.0224, nll_loss=0.0224, accuracy=0.9947, micro_step_time=0.0582, step_time=0.1191, lr=2.8515e-05, scale=1.0000
[2025-12-10 03:12:46] [INFO]  train | epoch 017:    1386 /  1429  global_step=24250, loss=0.0236, nll_loss=0.0236, accuracy=0.9947, micro_step_time=0.0579, step_time=0.1184, lr=2.7882e-05, scale=1.0000
[2025-12-10 03:12:52] [INFO]  End of epoch 17
[2025-12-10 03:12:52] [INFO]  train | epoch 017 | loss 0.0245 | nll_loss 0.0245 | kd_loss 0.0000
[2025-12-10 03:12:52] [INFO]  Evaluating before saving model...
[2025-12-10 03:12:52] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 03:13:57] [INFO]  eval_results in run@1: {'exact_match': 2.8, 'rougeL': 24.4092}
[2025-12-10 03:15:00] [INFO]  eval_results in run@2: {'exact_match': 3.2, 'rougeL': 24.1441}
[2025-12-10 03:16:00] [INFO]  eval_results in run@3: {'exact_match': 2.8, 'rougeL': 24.4584}
[2025-12-10 03:16:00] [INFO]  dev | {'loss': 6.285375, 'token_num': 75795, 'token_acc': 0.397942, 'top1_prob': 0.800686} | {'exact_match': 2.9333, 'rougeL': 24.3372}
[2025-12-10 03:16:00] [INFO]  Saving tokenizer...
[2025-12-10 03:16:01] [INFO]  Saving model...
[2025-12-10 03:16:01] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch17_step24293_loss6.2854_rougel24.3372
[2025-12-10 03:16:01] [INFO]  Start iterations of epoch 18
[2025-12-10 03:16:02] [INFO]  train | epoch 018:       7 /  1429  global_step=24300, loss=0.0237, nll_loss=0.0237, accuracy=0.9941, micro_step_time=0.0581, step_time=0.1190, lr=2.7256e-05, scale=1.0000
[2025-12-10 03:16:08] [INFO]  train | epoch 018:      57 /  1429  global_step=24350, loss=0.0198, nll_loss=0.0198, accuracy=0.9954, micro_step_time=0.0586, step_time=0.1199, lr=2.6636e-05, scale=1.0000
[2025-12-10 03:16:14] [INFO]  train | epoch 018:     107 /  1429  global_step=24400, loss=0.0201, nll_loss=0.0201, accuracy=0.9953, micro_step_time=0.0584, step_time=0.1196, lr=2.6024e-05, scale=1.0000
[2025-12-10 03:16:20] [INFO]  train | epoch 018:     157 /  1429  global_step=24450, loss=0.0199, nll_loss=0.0199, accuracy=0.9952, micro_step_time=0.0581, step_time=0.1189, lr=2.5418e-05, scale=1.0000
[2025-12-10 03:16:26] [INFO]  train | epoch 018:     207 /  1429  global_step=24500, loss=0.0188, nll_loss=0.0188, accuracy=0.9959, micro_step_time=0.0581, step_time=0.1189, lr=2.4819e-05, scale=1.0000
[2025-12-10 03:16:32] [INFO]  train | epoch 018:     257 /  1429  global_step=24550, loss=0.0203, nll_loss=0.0203, accuracy=0.9959, micro_step_time=0.0581, step_time=0.1190, lr=2.4227e-05, scale=1.0000
[2025-12-10 03:16:38] [INFO]  train | epoch 018:     307 /  1429  global_step=24600, loss=0.0200, nll_loss=0.0200, accuracy=0.9951, micro_step_time=0.0583, step_time=0.1193, lr=2.3641e-05, scale=1.0000
[2025-12-10 03:16:44] [INFO]  train | epoch 018:     357 /  1429  global_step=24650, loss=0.0205, nll_loss=0.0205, accuracy=0.9952, micro_step_time=0.0593, step_time=0.1214, lr=2.3062e-05, scale=1.0000
[2025-12-10 03:16:50] [INFO]  train | epoch 018:     407 /  1429  global_step=24700, loss=0.0175, nll_loss=0.0175, accuracy=0.9960, micro_step_time=0.0590, step_time=0.1207, lr=2.2491e-05, scale=1.0000
[2025-12-10 03:16:56] [INFO]  train | epoch 018:     457 /  1429  global_step=24750, loss=0.0190, nll_loss=0.0190, accuracy=0.9956, micro_step_time=0.0582, step_time=0.1190, lr=2.1926e-05, scale=1.0000
[2025-12-10 03:17:02] [INFO]  train | epoch 018:     507 /  1429  global_step=24800, loss=0.0209, nll_loss=0.0209, accuracy=0.9957, micro_step_time=0.0582, step_time=0.1190, lr=2.1368e-05, scale=1.0000
[2025-12-10 03:17:08] [INFO]  train | epoch 018:     557 /  1429  global_step=24850, loss=0.0208, nll_loss=0.0208, accuracy=0.9956, micro_step_time=0.0582, step_time=0.1190, lr=2.0817e-05, scale=1.0000
[2025-12-10 03:17:14] [INFO]  train | epoch 018:     607 /  1429  global_step=24900, loss=0.0203, nll_loss=0.0203, accuracy=0.9952, micro_step_time=0.0581, step_time=0.1190, lr=2.0273e-05, scale=1.0000
[2025-12-10 03:17:19] [INFO]  train | epoch 018:     657 /  1429  global_step=24950, loss=0.0186, nll_loss=0.0186, accuracy=0.9960, micro_step_time=0.0582, step_time=0.1190, lr=1.9735e-05, scale=1.0000
[2025-12-10 03:17:25] [INFO]  train | epoch 018:     707 /  1429  global_step=25000, loss=0.0172, nll_loss=0.0172, accuracy=0.9959, micro_step_time=0.0581, step_time=0.1189, lr=1.9205e-05, scale=1.0000
[2025-12-10 03:17:31] [INFO]  train | epoch 018:     757 /  1429  global_step=25050, loss=0.0207, nll_loss=0.0207, accuracy=0.9953, micro_step_time=0.0582, step_time=0.1191, lr=1.8682e-05, scale=1.0000
[2025-12-10 03:17:37] [INFO]  train | epoch 018:     807 /  1429  global_step=25100, loss=0.0179, nll_loss=0.0179, accuracy=0.9959, micro_step_time=0.0583, step_time=0.1192, lr=1.8166e-05, scale=1.0000
[2025-12-10 03:17:43] [INFO]  train | epoch 018:     857 /  1429  global_step=25150, loss=0.0209, nll_loss=0.0209, accuracy=0.9949, micro_step_time=0.0586, step_time=0.1199, lr=1.7656e-05, scale=1.0000
[2025-12-10 03:17:49] [INFO]  train | epoch 018:     907 /  1429  global_step=25200, loss=0.0220, nll_loss=0.0220, accuracy=0.9948, micro_step_time=0.0585, step_time=0.1198, lr=1.7154e-05, scale=1.0000
[2025-12-10 03:17:55] [INFO]  train | epoch 018:     957 /  1429  global_step=25250, loss=0.0187, nll_loss=0.0187, accuracy=0.9959, micro_step_time=0.0584, step_time=0.1194, lr=1.6659e-05, scale=1.0000
[2025-12-10 03:18:01] [INFO]  train | epoch 018:    1007 /  1429  global_step=25300, loss=0.0197, nll_loss=0.0197, accuracy=0.9954, micro_step_time=0.0582, step_time=0.1191, lr=1.6171e-05, scale=1.0000
[2025-12-10 03:18:07] [INFO]  train | epoch 018:    1057 /  1429  global_step=25350, loss=0.0208, nll_loss=0.0208, accuracy=0.9953, micro_step_time=0.0582, step_time=0.1190, lr=1.5690e-05, scale=1.0000
[2025-12-10 03:18:13] [INFO]  train | epoch 018:    1107 /  1429  global_step=25400, loss=0.0218, nll_loss=0.0218, accuracy=0.9949, micro_step_time=0.0581, step_time=0.1189, lr=1.5216e-05, scale=1.0000
[2025-12-10 03:18:19] [INFO]  train | epoch 018:    1157 /  1429  global_step=25450, loss=0.0207, nll_loss=0.0207, accuracy=0.9950, micro_step_time=0.0582, step_time=0.1191, lr=1.4749e-05, scale=1.0000
[2025-12-10 03:18:25] [INFO]  train | epoch 018:    1207 /  1429  global_step=25500, loss=0.0203, nll_loss=0.0203, accuracy=0.9949, micro_step_time=0.0582, step_time=0.1191, lr=1.4289e-05, scale=1.0000
[2025-12-10 03:18:31] [INFO]  train | epoch 018:    1257 /  1429  global_step=25550, loss=0.0205, nll_loss=0.0205, accuracy=0.9949, micro_step_time=0.0582, step_time=0.1192, lr=1.3836e-05, scale=1.0000
[2025-12-10 03:18:37] [INFO]  train | epoch 018:    1307 /  1429  global_step=25600, loss=0.0190, nll_loss=0.0190, accuracy=0.9957, micro_step_time=0.0582, step_time=0.1191, lr=1.3391e-05, scale=1.0000
[2025-12-10 03:18:43] [INFO]  train | epoch 018:    1357 /  1429  global_step=25650, loss=0.0218, nll_loss=0.0218, accuracy=0.9947, micro_step_time=0.0583, step_time=0.1192, lr=1.2952e-05, scale=1.0000
[2025-12-10 03:18:49] [INFO]  train | epoch 018:    1407 /  1429  global_step=25700, loss=0.0196, nll_loss=0.0196, accuracy=0.9951, micro_step_time=0.0583, step_time=0.1192, lr=1.2521e-05, scale=1.0000
[2025-12-10 03:18:52] [INFO]  End of epoch 18
[2025-12-10 03:18:52] [INFO]  train | epoch 018 | loss 0.0204 | nll_loss 0.0204 | kd_loss 0.0000
[2025-12-10 03:18:52] [INFO]  Evaluating before saving model...
[2025-12-10 03:18:52] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 03:19:56] [INFO]  eval_results in run@1: {'exact_match': 2.7, 'rougeL': 24.7756}
[2025-12-10 03:20:57] [INFO]  eval_results in run@2: {'exact_match': 3.1, 'rougeL': 23.8974}
[2025-12-10 03:22:00] [INFO]  eval_results in run@3: {'exact_match': 3.0, 'rougeL': 24.747}
[2025-12-10 03:22:00] [INFO]  dev | {'loss': 6.298885, 'token_num': 75795, 'token_acc': 0.399076, 'top1_prob': 0.801346} | {'exact_match': 2.9333, 'rougeL': 24.4733}
[2025-12-10 03:22:00] [INFO]  Saving tokenizer...
[2025-12-10 03:22:00] [INFO]  Saving model...
[2025-12-10 03:22:01] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch18_step25722_loss6.2989_rougel24.4733
[2025-12-10 03:22:01] [INFO]  Start iterations of epoch 19
[2025-12-10 03:22:04] [INFO]  train | epoch 019:      28 /  1429  global_step=25750, loss=0.0188, nll_loss=0.0188, accuracy=0.9957, micro_step_time=0.0581, step_time=0.1189, lr=1.2097e-05, scale=1.0000
[2025-12-10 03:22:10] [INFO]  train | epoch 019:      78 /  1429  global_step=25800, loss=0.0167, nll_loss=0.0167, accuracy=0.9966, micro_step_time=0.0582, step_time=0.1191, lr=1.1680e-05, scale=1.0000
[2025-12-10 03:22:16] [INFO]  train | epoch 019:     128 /  1429  global_step=25850, loss=0.0170, nll_loss=0.0170, accuracy=0.9964, micro_step_time=0.0582, step_time=0.1191, lr=1.1270e-05, scale=1.0000
[2025-12-10 03:22:22] [INFO]  train | epoch 019:     178 /  1429  global_step=25900, loss=0.0164, nll_loss=0.0164, accuracy=0.9964, micro_step_time=0.0583, step_time=0.1193, lr=1.0868e-05, scale=1.0000
[2025-12-10 03:22:28] [INFO]  train | epoch 019:     228 /  1429  global_step=25950, loss=0.0172, nll_loss=0.0172, accuracy=0.9965, micro_step_time=0.0582, step_time=0.1191, lr=1.0472e-05, scale=1.0000
[2025-12-10 03:22:34] [INFO]  train | epoch 019:     278 /  1429  global_step=26000, loss=0.0172, nll_loss=0.0172, accuracy=0.9968, micro_step_time=0.0582, step_time=0.1191, lr=1.0084e-05, scale=1.0000
[2025-12-10 03:22:40] [INFO]  train | epoch 019:     328 /  1429  global_step=26050, loss=0.0173, nll_loss=0.0173, accuracy=0.9959, micro_step_time=0.0583, step_time=0.1193, lr=9.7037e-06, scale=1.0000
[2025-12-10 03:22:46] [INFO]  train | epoch 019:     378 /  1429  global_step=26100, loss=0.0183, nll_loss=0.0183, accuracy=0.9958, micro_step_time=0.0583, step_time=0.1193, lr=9.3302e-06, scale=1.0000
[2025-12-10 03:22:52] [INFO]  train | epoch 019:     428 /  1429  global_step=26150, loss=0.0165, nll_loss=0.0165, accuracy=0.9964, micro_step_time=0.0582, step_time=0.1191, lr=8.9639e-06, scale=1.0000
[2025-12-10 03:22:58] [INFO]  train | epoch 019:     478 /  1429  global_step=26200, loss=0.0160, nll_loss=0.0160, accuracy=0.9963, micro_step_time=0.0582, step_time=0.1192, lr=8.6050e-06, scale=1.0000
[2025-12-10 03:23:04] [INFO]  train | epoch 019:     528 /  1429  global_step=26250, loss=0.0163, nll_loss=0.0163, accuracy=0.9968, micro_step_time=0.0582, step_time=0.1191, lr=8.2533e-06, scale=1.0000
[2025-12-10 03:23:10] [INFO]  train | epoch 019:     578 /  1429  global_step=26300, loss=0.0174, nll_loss=0.0174, accuracy=0.9959, micro_step_time=0.0582, step_time=0.1192, lr=7.9090e-06, scale=1.0000
[2025-12-10 03:23:16] [INFO]  train | epoch 019:     628 /  1429  global_step=26350, loss=0.0170, nll_loss=0.0170, accuracy=0.9958, micro_step_time=0.0582, step_time=0.1190, lr=7.5719e-06, scale=1.0000
[2025-12-10 03:23:22] [INFO]  train | epoch 019:     678 /  1429  global_step=26400, loss=0.0159, nll_loss=0.0159, accuracy=0.9963, micro_step_time=0.0582, step_time=0.1191, lr=7.2422e-06, scale=1.0000
[2025-12-10 03:23:27] [INFO]  train | epoch 019:     728 /  1429  global_step=26450, loss=0.0181, nll_loss=0.0181, accuracy=0.9962, micro_step_time=0.0582, step_time=0.1190, lr=6.9198e-06, scale=1.0000
[2025-12-10 03:23:33] [INFO]  train | epoch 019:     778 /  1429  global_step=26500, loss=0.0172, nll_loss=0.0172, accuracy=0.9960, micro_step_time=0.0583, step_time=0.1192, lr=6.6048e-06, scale=1.0000
[2025-12-10 03:23:39] [INFO]  train | epoch 019:     828 /  1429  global_step=26550, loss=0.0154, nll_loss=0.0154, accuracy=0.9965, micro_step_time=0.0583, step_time=0.1192, lr=6.2971e-06, scale=1.0000
[2025-12-10 03:23:45] [INFO]  train | epoch 019:     878 /  1429  global_step=26600, loss=0.0153, nll_loss=0.0153, accuracy=0.9974, micro_step_time=0.0582, step_time=0.1192, lr=5.9968e-06, scale=1.0000
[2025-12-10 03:23:51] [INFO]  train | epoch 019:     928 /  1429  global_step=26650, loss=0.0161, nll_loss=0.0161, accuracy=0.9964, micro_step_time=0.0583, step_time=0.1192, lr=5.7038e-06, scale=1.0000
[2025-12-10 03:23:57] [INFO]  train | epoch 019:     978 /  1429  global_step=26700, loss=0.0163, nll_loss=0.0163, accuracy=0.9964, micro_step_time=0.0582, step_time=0.1191, lr=5.4182e-06, scale=1.0000
[2025-12-10 03:24:03] [INFO]  train | epoch 019:    1028 /  1429  global_step=26750, loss=0.0172, nll_loss=0.0172, accuracy=0.9965, micro_step_time=0.0583, step_time=0.1193, lr=5.1401e-06, scale=1.0000
[2025-12-10 03:24:09] [INFO]  train | epoch 019:    1078 /  1429  global_step=26800, loss=0.0170, nll_loss=0.0170, accuracy=0.9961, micro_step_time=0.0582, step_time=0.1191, lr=4.8693e-06, scale=1.0000
[2025-12-10 03:24:15] [INFO]  train | epoch 019:    1128 /  1429  global_step=26850, loss=0.0174, nll_loss=0.0174, accuracy=0.9964, micro_step_time=0.0582, step_time=0.1191, lr=4.6059e-06, scale=1.0000
[2025-12-10 03:24:21] [INFO]  train | epoch 019:    1178 /  1429  global_step=26900, loss=0.0174, nll_loss=0.0174, accuracy=0.9960, micro_step_time=0.0582, step_time=0.1192, lr=4.3499e-06, scale=1.0000
[2025-12-10 03:24:27] [INFO]  train | epoch 019:    1228 /  1429  global_step=26950, loss=0.0176, nll_loss=0.0176, accuracy=0.9959, micro_step_time=0.0582, step_time=0.1191, lr=4.1014e-06, scale=1.0000
[2025-12-10 03:24:33] [INFO]  train | epoch 019:    1278 /  1429  global_step=27000, loss=0.0179, nll_loss=0.0179, accuracy=0.9961, micro_step_time=0.0582, step_time=0.1190, lr=3.8603e-06, scale=1.0000
[2025-12-10 03:24:39] [INFO]  train | epoch 019:    1328 /  1429  global_step=27050, loss=0.0187, nll_loss=0.0187, accuracy=0.9956, micro_step_time=0.0582, step_time=0.1191, lr=3.6266e-06, scale=1.0000
[2025-12-10 03:24:45] [INFO]  train | epoch 019:    1378 /  1429  global_step=27100, loss=0.0178, nll_loss=0.0178, accuracy=0.9960, micro_step_time=0.0582, step_time=0.1191, lr=3.4004e-06, scale=1.0000
[2025-12-10 03:24:51] [INFO]  train | epoch 019:    1428 /  1429  global_step=27150, loss=0.0152, nll_loss=0.0152, accuracy=0.9965, micro_step_time=0.0582, step_time=0.1191, lr=3.1816e-06, scale=1.0000
[2025-12-10 03:24:51] [INFO]  End of epoch 19
[2025-12-10 03:24:51] [INFO]  train | epoch 019 | loss 0.0172 | nll_loss 0.0172 | kd_loss 0.0000
[2025-12-10 03:24:51] [INFO]  Evaluating before saving model...
[2025-12-10 03:24:51] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 03:25:55] [INFO]  eval_results in run@1: {'exact_match': 3.3, 'rougeL': 24.382}
[2025-12-10 03:26:58] [INFO]  eval_results in run@2: {'exact_match': 3.3, 'rougeL': 24.4442}
[2025-12-10 03:28:01] [INFO]  eval_results in run@3: {'exact_match': 3.3, 'rougeL': 24.0975}
[2025-12-10 03:28:01] [INFO]  dev | {'loss': 6.310073, 'token_num': 75795, 'token_acc': 0.399565, 'top1_prob': 0.802744} | {'exact_match': 3.3, 'rougeL': 24.3079}
[2025-12-10 03:28:01] [INFO]  Saving tokenizer...
[2025-12-10 03:28:01] [INFO]  Saving model...
[2025-12-10 03:28:01] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch19_step27151_loss6.3101_rougel24.3079
[2025-12-10 03:28:01] [INFO]  Start iterations of epoch 20
[2025-12-10 03:28:07] [INFO]  train | epoch 020:      49 /  1429  global_step=27200, loss=0.0159, nll_loss=0.0159, accuracy=0.9966, micro_step_time=0.0579, step_time=0.1185, lr=2.9703e-06, scale=1.0000
[2025-12-10 03:28:13] [INFO]  train | epoch 020:      99 /  1429  global_step=27250, loss=0.0170, nll_loss=0.0170, accuracy=0.9966, micro_step_time=0.0560, step_time=0.1145, lr=2.7664e-06, scale=1.0000
[2025-12-10 03:28:19] [INFO]  train | epoch 020:     149 /  1429  global_step=27300, loss=0.0148, nll_loss=0.0148, accuracy=0.9969, micro_step_time=0.0592, step_time=0.1212, lr=2.5700e-06, scale=1.0000
[2025-12-10 03:28:25] [INFO]  train | epoch 020:     199 /  1429  global_step=27350, loss=0.0147, nll_loss=0.0147, accuracy=0.9969, micro_step_time=0.0591, step_time=0.1210, lr=2.3811e-06, scale=1.0000
[2025-12-10 03:28:31] [INFO]  train | epoch 020:     249 /  1429  global_step=27400, loss=0.0142, nll_loss=0.0142, accuracy=0.9973, micro_step_time=0.0582, step_time=0.1192, lr=2.1997e-06, scale=1.0000
[2025-12-10 03:28:37] [INFO]  train | epoch 020:     299 /  1429  global_step=27450, loss=0.0169, nll_loss=0.0169, accuracy=0.9957, micro_step_time=0.0581, step_time=0.1190, lr=2.0257e-06, scale=1.0000
[2025-12-10 03:28:43] [INFO]  train | epoch 020:     349 /  1429  global_step=27500, loss=0.0152, nll_loss=0.0152, accuracy=0.9963, micro_step_time=0.0581, step_time=0.1190, lr=1.8593e-06, scale=1.0000
[2025-12-10 03:28:49] [INFO]  train | epoch 020:     399 /  1429  global_step=27550, loss=0.0151, nll_loss=0.0151, accuracy=0.9969, micro_step_time=0.0582, step_time=0.1191, lr=1.7003e-06, scale=1.0000
[2025-12-10 03:28:55] [INFO]  train | epoch 020:     449 /  1429  global_step=27600, loss=0.0154, nll_loss=0.0154, accuracy=0.9970, micro_step_time=0.0582, step_time=0.1191, lr=1.5489e-06, scale=1.0000
[2025-12-10 03:29:01] [INFO]  train | epoch 020:     499 /  1429  global_step=27650, loss=0.0148, nll_loss=0.0148, accuracy=0.9972, micro_step_time=0.0582, step_time=0.1191, lr=1.4049e-06, scale=1.0000
[2025-12-10 03:29:07] [INFO]  train | epoch 020:     549 /  1429  global_step=27700, loss=0.0179, nll_loss=0.0179, accuracy=0.9958, micro_step_time=0.0582, step_time=0.1192, lr=1.2685e-06, scale=1.0000
[2025-12-10 03:29:12] [INFO]  train | epoch 020:     599 /  1429  global_step=27750, loss=0.0150, nll_loss=0.0150, accuracy=0.9973, micro_step_time=0.0581, step_time=0.1190, lr=1.1396e-06, scale=1.0000
[2025-12-10 03:29:18] [INFO]  train | epoch 020:     649 /  1429  global_step=27800, loss=0.0168, nll_loss=0.0168, accuracy=0.9962, micro_step_time=0.0582, step_time=0.1190, lr=1.0182e-06, scale=1.0000
[2025-12-10 03:29:24] [INFO]  train | epoch 020:     699 /  1429  global_step=27850, loss=0.0153, nll_loss=0.0153, accuracy=0.9967, micro_step_time=0.0581, step_time=0.1190, lr=9.0429e-07, scale=1.0000
[2025-12-10 03:29:30] [INFO]  train | epoch 020:     749 /  1429  global_step=27900, loss=0.0156, nll_loss=0.0156, accuracy=0.9965, micro_step_time=0.0582, step_time=0.1191, lr=7.9793e-07, scale=1.0000
[2025-12-10 03:29:36] [INFO]  train | epoch 020:     799 /  1429  global_step=27950, loss=0.0175, nll_loss=0.0175, accuracy=0.9962, micro_step_time=0.0582, step_time=0.1191, lr=6.9911e-07, scale=1.0000
[2025-12-10 03:29:42] [INFO]  train | epoch 020:     849 /  1429  global_step=28000, loss=0.0135, nll_loss=0.0135, accuracy=0.9974, micro_step_time=0.0582, step_time=0.1190, lr=6.0782e-07, scale=1.0000
[2025-12-10 03:29:48] [INFO]  train | epoch 020:     899 /  1429  global_step=28050, loss=0.0144, nll_loss=0.0144, accuracy=0.9971, micro_step_time=0.0582, step_time=0.1190, lr=5.2406e-07, scale=1.0000
[2025-12-10 03:29:54] [INFO]  train | epoch 020:     949 /  1429  global_step=28100, loss=0.0154, nll_loss=0.0154, accuracy=0.9965, micro_step_time=0.0583, step_time=0.1192, lr=4.4784e-07, scale=1.0000
[2025-12-10 03:30:00] [INFO]  train | epoch 020:     999 /  1429  global_step=28150, loss=0.0168, nll_loss=0.0168, accuracy=0.9963, micro_step_time=0.0583, step_time=0.1193, lr=3.7916e-07, scale=1.0000
[2025-12-10 03:30:06] [INFO]  train | epoch 020:    1049 /  1429  global_step=28200, loss=0.0149, nll_loss=0.0149, accuracy=0.9970, micro_step_time=0.0582, step_time=0.1191, lr=3.1802e-07, scale=1.0000
[2025-12-10 03:30:12] [INFO]  train | epoch 020:    1099 /  1429  global_step=28250, loss=0.0144, nll_loss=0.0144, accuracy=0.9969, micro_step_time=0.0582, step_time=0.1190, lr=2.6443e-07, scale=1.0000
[2025-12-10 03:30:18] [INFO]  train | epoch 020:    1149 /  1429  global_step=28300, loss=0.0170, nll_loss=0.0170, accuracy=0.9958, micro_step_time=0.0582, step_time=0.1190, lr=2.1838e-07, scale=1.0000
[2025-12-10 03:30:24] [INFO]  train | epoch 020:    1199 /  1429  global_step=28350, loss=0.0162, nll_loss=0.0162, accuracy=0.9961, micro_step_time=0.0582, step_time=0.1191, lr=1.7988e-07, scale=1.0000
[2025-12-10 03:30:30] [INFO]  train | epoch 020:    1249 /  1429  global_step=28400, loss=0.0152, nll_loss=0.0152, accuracy=0.9967, micro_step_time=0.0582, step_time=0.1191, lr=1.4892e-07, scale=1.0000
[2025-12-10 03:30:36] [INFO]  train | epoch 020:    1299 /  1429  global_step=28450, loss=0.0170, nll_loss=0.0170, accuracy=0.9964, micro_step_time=0.0582, step_time=0.1191, lr=1.2552e-07, scale=1.0000
[2025-12-10 03:30:42] [INFO]  train | epoch 020:    1349 /  1429  global_step=28500, loss=0.0148, nll_loss=0.0148, accuracy=0.9969, micro_step_time=0.0582, step_time=0.1191, lr=1.0966e-07, scale=1.0000
[2025-12-10 03:30:48] [INFO]  train | epoch 020:    1399 /  1429  global_step=28550, loss=0.0140, nll_loss=0.0140, accuracy=0.9970, micro_step_time=0.0582, step_time=0.1191, lr=1.0136e-07, scale=1.0000
[2025-12-10 03:30:51] [INFO]  End of epoch 20
[2025-12-10 03:30:51] [INFO]  train | epoch 020 | loss 0.0152 | nll_loss 0.0152 | kd_loss 0.0000
[2025-12-10 03:30:51] [INFO]  Evaluating before saving model...
[2025-12-10 03:30:51] [INFO]  Evaluating on dev set with 1 GPU(s)
[2025-12-10 03:31:55] [INFO]  eval_results in run@1: {'exact_match': 3.2, 'rougeL': 24.7444}
[2025-12-10 03:32:57] [INFO]  eval_results in run@2: {'exact_match': 3.2, 'rougeL': 24.7835}
[2025-12-10 03:34:00] [INFO]  eval_results in run@3: {'exact_match': 3.1, 'rougeL': 24.3835}
[2025-12-10 03:34:00] [INFO]  dev | {'loss': 6.312606, 'token_num': 75795, 'token_acc': 0.399446, 'top1_prob': 0.803061} | {'exact_match': 3.1667, 'rougeL': 24.6371}
[2025-12-10 03:34:00] [INFO]  Saving tokenizer...
[2025-12-10 03:34:00] [INFO]  Saving model...
[2025-12-10 03:34:01] [INFO]  Model has been saved to /workspace/DSKD/outputs/gpt2/gpt2-base/sft/criterion=cross_entropy__default-bf16__epoch=20__bsz=4x2x1=8__lr=0.0005/epoch20_step28580_loss6.3126_rougel24.6371
[2025-12-10 03:34:01] [INFO]  Done training in 02:01:56
[rank0]:[W1210 03:34:01.906991887 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
